{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "from mutimodal_lib_ablation import AblationCodeTx \n",
    "\n",
    "BASE_PROJECT_PATH = \"OctopusGuard/evaluations/ablation_and_training_progress\"\n",
    "BASE_MODEL_PATH = \"OctopusGuard/src/qwen25vl_7b_finetune_checkpoint\"\n",
    "\n",
    "ADDRESS_CSV_PATH = \"OctopusGuard/data/experiment_address.csv\"\n",
    "CONTRACT_DIR = \"OctopusGuard/data/token_contracts\"\n",
    "TRANSFER_DIR = \"OctopusGuard/data/token_transfer_files\"\n",
    "\n",
    "LOG_DIR = os.path.join(BASE_PROJECT_PATH, \"logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "checkpoints_steps = [74, 148, 222, 296] \n",
    "try:\n",
    "    contract_df = pd.read_csv(ADDRESS_CSV_PATH)\n",
    "    contract_addresses = contract_df['contract_address'].tolist()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Address file not found at {ADDRESS_CSV_PATH}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "for step in checkpoints_steps:\n",
    "    model_id = os.path.join(BASE_MODEL_PATH, f\"checkpoint-{step}\")\n",
    "    log_file = os.path.join(LOG_DIR, f\"ablation_code_tx_log_checkpoint-{step}.txt\")\n",
    "\n",
    "    print(f\"\\n{'='*20} STARTING ABLATION (Code+Tx) EVALUATION FOR CHECKPOINT: checkpoint-{step} {'='*20}\")\n",
    "    print(f\"Model Path: {model_id}\")\n",
    "    print(f\"Log will be saved to: {log_file}\")\n",
    "\n",
    "    if not os.path.isdir(model_id):\n",
    "        print(f\"⚠️ Checkpoint directory not found, skipping: {model_id}\")\n",
    "        continue\n",
    "    \n",
    "    model = None \n",
    "    try:\n",
    "        model = AblationCodeTx(model_id=model_id)\n",
    "\n",
    "        for idx, address in enumerate(contract_addresses):\n",
    "            print(f\"\\n----- Analyzing address {idx+1}/{len(contract_addresses)}: {address} (with checkpoint-{step} [Ablation]) -----\")\n",
    "\n",
    "            norm_address = address.lower()\n",
    "            contract_path = next((os.path.join(CONTRACT_DIR, f) for f in os.listdir(CONTRACT_DIR) if f.lower().startswith(norm_address)), None)\n",
    "            csv_path = next((os.path.join(TRANSFER_DIR, f) for f in os.listdir(TRANSFER_DIR) if f.lower().startswith(norm_address)), None)\n",
    "            \n",
    "            if not all([contract_path, csv_path]):\n",
    "                print(f\"⚠️ Missing contract or csv file for address, skipping: {address}\")\n",
    "                with open(log_file, \"a\", encoding=\"utf-8\") as logf:\n",
    "                    logf.write(f\"\\n--- Address: {address} --- SKIPPED (Missing Files) ---\\n\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = model.unified_analysis(contract_path, csv_path)\n",
    "\n",
    "                with open(log_file, \"a\", encoding=\"utf-8\") as logf:\n",
    "                    logf.write(f\"\\n==================== Contract Address: {address} ====================\\n\")\n",
    "                    logf.write(result + \"\\n\")\n",
    "                    logf.write(\"====================== ANALYSIS COMPLETE ======================\\n\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Analysis failed for address {address}: {str(e)}\")\n",
    "                with open(log_file, \"a\", encoding=\"utf-8\") as logf:\n",
    "                    logf.write(f\"\\n==================== Contract Address: {address} ====================\\n\")\n",
    "                    logf.write(f\"Analysis failed with error: {str(e)}\\n\")\n",
    "                    logf.write(\"====================== ANALYSIS FAILED ======================\\n\\n\")\n",
    "            \n",
    "            time.sleep(1) \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ CRITICAL ERROR: Failed to initialize or run for checkpoint-{step}. Error: {str(e)}\")\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as logf:\n",
    "            logf.write(f\"\\nCRITICAL ERROR for checkpoint-{step}: {str(e)}\\n\")\n",
    "\n",
    "    finally:\n",
    "        if model:\n",
    "            del model\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU memory cleared for next run.\")\n",
    "        print(f\"{'='*20} FINISHED ABLATION EVALUATION FOR CHECKPOINT: checkpoint-{step} {'='*20}\\n\")\n",
    "        time.sleep(5) \n",
    "\n",
    "print(\"\\n===== ALL ABLATION (Code+Tx) CHECKPOINTS EVALUATED =====\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
