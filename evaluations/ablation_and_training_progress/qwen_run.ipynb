{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append('OctopusGuard/src')\n",
    "from mutimodal_qwen_lib import Multimodal\n",
    "\n",
    "BASE_PROJECT_PATH = \"OctopusGuard/evaluations/ablation_and_training_progress\"\n",
    "BASE_MODEL_PATH = \"OctopusGuard/src/qwen25vl_7b_finetune_checkpoint\"\n",
    "\n",
    "CLIP_PATH = \"openai/clip-vit-base-patch32\"\n",
    "VECTOR_DB_PATH = \"OctopusGuard/data_processing/klines_rag_db/vector_db/klines_faiss.index\"\n",
    "META_PATH = \"OctopusGuard/data_processing/klines_rag_db/vector_db/metadata.pkl\"\n",
    "\n",
    "ADDRESS_CSV_PATH = \"OctopusGuard/data/experiment_address.csv\"\n",
    "CONTRACT_DIR = \"OctopusGuard/data/token_contracts\"\n",
    "KLINE_DIR = \"OctopusGuard/data/token_kline_images\"\n",
    "TRANSFER_DIR = \"OctopusGuard/data/token_transfer_files\"\n",
    "\n",
    "LOG_DIR = os.path.join(BASE_PROJECT_PATH, \"logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "checkpoints_steps = [0, 74, 148, 222, 296]\n",
    "try:\n",
    "    contract_df = pd.read_csv(ADDRESS_CSV_PATH)\n",
    "    contract_addresses = contract_df['contract_address'].tolist()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Address file not found at {ADDRESS_CSV_PATH}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "for step in checkpoints_steps:\n",
    "    model_id = os.path.join(BASE_MODEL_PATH, f\"checkpoint-{step}\")\n",
    "    log_file = os.path.join(LOG_DIR, f\"analysis_log_checkpoint-{step}.txt\")\n",
    "\n",
    "    print(f\"\\n{'='*20} STARTING EVALUATION FOR CHECKPOINT: checkpoint-{step} {'='*20}\")\n",
    "    print(f\"Model Path: {model_id}\")\n",
    "    print(f\"Log will be saved to: {log_file}\")\n",
    "\n",
    "    if not os.path.isdir(model_id):\n",
    "        print(f\"⚠️ Checkpoint directory not found, skipping: {model_id}\")\n",
    "        continue\n",
    "    \n",
    "    model = None \n",
    "    try:\n",
    "        model = Multimodal(\n",
    "            model_id=model_id,\n",
    "            clip_path=CLIP_PATH,\n",
    "            vector_db_path=VECTOR_DB_PATH,\n",
    "            meta_path=META_PATH\n",
    "        )\n",
    "\n",
    "        for idx, address in enumerate(contract_addresses):\n",
    "            print(f\"\\n----- Analyzing address {idx+1}/{len(contract_addresses)}: {address} (with checkpoint-{step}) -----\")\n",
    "\n",
    "            norm_address = address.lower()\n",
    "            contract_path = next((os.path.join(CONTRACT_DIR, f) for f in os.listdir(CONTRACT_DIR) if f.lower().startswith(norm_address)), None)\n",
    "            image_path = next((os.path.join(KLINE_DIR, f) for f in os.listdir(KLINE_DIR) if f.lower().startswith(norm_address)), None)\n",
    "            csv_path = next((os.path.join(TRANSFER_DIR, f) for f in os.listdir(TRANSFER_DIR) if f.lower().startswith(norm_address)), None)\n",
    "\n",
    "            if not all([contract_path, image_path, csv_path]):\n",
    "                print(f\"⚠️ Missing files for address, skipping: {address}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = model.unified_analysis(contract_path, csv_path, image_path)\n",
    "\n",
    "                with open(log_file, \"a\", encoding=\"utf-8\") as logf:\n",
    "                    logf.write(f\"\\n==================== Contract Address: {address} ====================\\n\")\n",
    "                    logf.write(result + \"\\n\")\n",
    "                    logf.write(\"====================== ANALYSIS COMPLETE ======================\\n\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Analysis failed for address {address}: {str(e)}\")\n",
    "                with open(log_file, \"a\", encoding=\"utf-8\") as logf:\n",
    "                    logf.write(f\"\\n==================== Contract Address: {address} ====================\\n\")\n",
    "                    logf.write(f\"Analysis failed with error: {str(e)}\\n\")\n",
    "                    logf.write(\"====================== ANALYSIS FAILED ======================\\n\\n\")\n",
    "            \n",
    "            time.sleep(1) \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ CRITICAL ERROR: Failed to initialize or run for checkpoint-{step}. Error: {str(e)}\")\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as logf:\n",
    "            logf.write(f\"\\nCRITICAL ERROR for checkpoint-{step}: {str(e)}\\n\")\n",
    "\n",
    "    finally:\n",
    "        if model:\n",
    "            del model\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU memory cleared for next run.\")\n",
    "        print(f\"{'='*20} FINISHED EVALUATION FOR CHECKPOINT: checkpoint-{step} {'='*20}\\n\")\n",
    "        time.sleep(5) \n",
    "\n",
    "print(\"\\n===== ALL CHECKPOINTS EVALUATED =====\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
