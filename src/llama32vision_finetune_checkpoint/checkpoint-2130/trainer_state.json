{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 30.0,
  "eval_steps": 500,
  "global_step": 2130,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1413427561837456,
      "grad_norm": 0.34885042905807495,
      "learning_rate": 1.8000000000000001e-06,
      "loss": 1.2581,
      "step": 10
    },
    {
      "epoch": 0.2826855123674912,
      "grad_norm": 0.46383604407310486,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 1.2562,
      "step": 20
    },
    {
      "epoch": 0.42402826855123676,
      "grad_norm": 0.3793405294418335,
      "learning_rate": 5.8e-06,
      "loss": 1.2424,
      "step": 30
    },
    {
      "epoch": 0.5653710247349824,
      "grad_norm": 0.3035266697406769,
      "learning_rate": 7.800000000000002e-06,
      "loss": 1.1958,
      "step": 40
    },
    {
      "epoch": 0.7067137809187279,
      "grad_norm": 0.3132104277610779,
      "learning_rate": 9.800000000000001e-06,
      "loss": 1.1705,
      "step": 50
    },
    {
      "epoch": 0.8480565371024735,
      "grad_norm": 0.33203741908073425,
      "learning_rate": 1.18e-05,
      "loss": 1.0972,
      "step": 60
    },
    {
      "epoch": 0.9893992932862191,
      "grad_norm": 0.309099018573761,
      "learning_rate": 1.38e-05,
      "loss": 0.9858,
      "step": 70
    },
    {
      "epoch": 1.127208480565371,
      "grad_norm": 0.36514803767204285,
      "learning_rate": 1.58e-05,
      "loss": 0.9035,
      "step": 80
    },
    {
      "epoch": 1.2685512367491167,
      "grad_norm": 0.27710384130477905,
      "learning_rate": 1.7800000000000002e-05,
      "loss": 0.6967,
      "step": 90
    },
    {
      "epoch": 1.4098939929328622,
      "grad_norm": 0.16531293094158173,
      "learning_rate": 1.98e-05,
      "loss": 0.6904,
      "step": 100
    },
    {
      "epoch": 1.5512367491166077,
      "grad_norm": 0.16087576746940613,
      "learning_rate": 1.9911330049261087e-05,
      "loss": 0.6423,
      "step": 110
    },
    {
      "epoch": 1.6925795053003534,
      "grad_norm": 0.17073732614517212,
      "learning_rate": 1.98128078817734e-05,
      "loss": 0.5999,
      "step": 120
    },
    {
      "epoch": 1.833922261484099,
      "grad_norm": 0.17081661522388458,
      "learning_rate": 1.9714285714285718e-05,
      "loss": 0.6122,
      "step": 130
    },
    {
      "epoch": 1.9752650176678446,
      "grad_norm": 0.15379035472869873,
      "learning_rate": 1.961576354679803e-05,
      "loss": 0.5786,
      "step": 140
    },
    {
      "epoch": 2.1130742049469964,
      "grad_norm": 0.17304649949073792,
      "learning_rate": 1.9517241379310345e-05,
      "loss": 0.5301,
      "step": 150
    },
    {
      "epoch": 2.254416961130742,
      "grad_norm": 0.17029428482055664,
      "learning_rate": 1.9418719211822662e-05,
      "loss": 0.5634,
      "step": 160
    },
    {
      "epoch": 2.395759717314488,
      "grad_norm": 0.18791961669921875,
      "learning_rate": 1.9320197044334975e-05,
      "loss": 0.4922,
      "step": 170
    },
    {
      "epoch": 2.5371024734982335,
      "grad_norm": 0.21410514414310455,
      "learning_rate": 1.9221674876847292e-05,
      "loss": 0.5273,
      "step": 180
    },
    {
      "epoch": 2.6784452296819787,
      "grad_norm": 0.2635267674922943,
      "learning_rate": 1.9123152709359606e-05,
      "loss": 0.4607,
      "step": 190
    },
    {
      "epoch": 2.8197879858657244,
      "grad_norm": 0.3091670274734497,
      "learning_rate": 1.9024630541871923e-05,
      "loss": 0.4497,
      "step": 200
    },
    {
      "epoch": 2.9611307420494697,
      "grad_norm": 0.25060245394706726,
      "learning_rate": 1.8926108374384236e-05,
      "loss": 0.4371,
      "step": 210
    },
    {
      "epoch": 3.0989399293286217,
      "grad_norm": 0.32406288385391235,
      "learning_rate": 1.8827586206896553e-05,
      "loss": 0.4284,
      "step": 220
    },
    {
      "epoch": 3.2402826855123674,
      "grad_norm": 0.37635165452957153,
      "learning_rate": 1.872906403940887e-05,
      "loss": 0.3735,
      "step": 230
    },
    {
      "epoch": 3.381625441696113,
      "grad_norm": 0.3683488070964813,
      "learning_rate": 1.8630541871921184e-05,
      "loss": 0.3693,
      "step": 240
    },
    {
      "epoch": 3.522968197879859,
      "grad_norm": 0.32656705379486084,
      "learning_rate": 1.85320197044335e-05,
      "loss": 0.2942,
      "step": 250
    },
    {
      "epoch": 3.664310954063604,
      "grad_norm": 0.4134601354598999,
      "learning_rate": 1.8433497536945814e-05,
      "loss": 0.3241,
      "step": 260
    },
    {
      "epoch": 3.8056537102473498,
      "grad_norm": 0.4259989857673645,
      "learning_rate": 1.833497536945813e-05,
      "loss": 0.2993,
      "step": 270
    },
    {
      "epoch": 3.9469964664310955,
      "grad_norm": 0.5436468124389648,
      "learning_rate": 1.8236453201970445e-05,
      "loss": 0.2616,
      "step": 280
    },
    {
      "epoch": 4.084805653710247,
      "grad_norm": 0.6092093586921692,
      "learning_rate": 1.813793103448276e-05,
      "loss": 0.2478,
      "step": 290
    },
    {
      "epoch": 4.226148409893993,
      "grad_norm": 0.48983803391456604,
      "learning_rate": 1.8039408866995075e-05,
      "loss": 0.1978,
      "step": 300
    },
    {
      "epoch": 4.3674911660777385,
      "grad_norm": 0.47841352224349976,
      "learning_rate": 1.7940886699507392e-05,
      "loss": 0.2068,
      "step": 310
    },
    {
      "epoch": 4.508833922261484,
      "grad_norm": 0.5043819546699524,
      "learning_rate": 1.7842364532019706e-05,
      "loss": 0.1964,
      "step": 320
    },
    {
      "epoch": 4.65017667844523,
      "grad_norm": 0.7227679491043091,
      "learning_rate": 1.774384236453202e-05,
      "loss": 0.1733,
      "step": 330
    },
    {
      "epoch": 4.791519434628976,
      "grad_norm": 0.5332374572753906,
      "learning_rate": 1.7645320197044336e-05,
      "loss": 0.1763,
      "step": 340
    },
    {
      "epoch": 4.93286219081272,
      "grad_norm": 0.3898160457611084,
      "learning_rate": 1.754679802955665e-05,
      "loss": 0.125,
      "step": 350
    },
    {
      "epoch": 5.070671378091872,
      "grad_norm": 0.5378025770187378,
      "learning_rate": 1.7448275862068966e-05,
      "loss": 0.1635,
      "step": 360
    },
    {
      "epoch": 5.212014134275618,
      "grad_norm": 0.47214701771736145,
      "learning_rate": 1.734975369458128e-05,
      "loss": 0.1178,
      "step": 370
    },
    {
      "epoch": 5.353356890459364,
      "grad_norm": 0.47531577944755554,
      "learning_rate": 1.7251231527093597e-05,
      "loss": 0.117,
      "step": 380
    },
    {
      "epoch": 5.4946996466431095,
      "grad_norm": 0.3916400372982025,
      "learning_rate": 1.7152709359605914e-05,
      "loss": 0.1196,
      "step": 390
    },
    {
      "epoch": 5.636042402826855,
      "grad_norm": 0.648095965385437,
      "learning_rate": 1.7054187192118227e-05,
      "loss": 0.0995,
      "step": 400
    },
    {
      "epoch": 5.777385159010601,
      "grad_norm": 0.3974827229976654,
      "learning_rate": 1.6955665024630544e-05,
      "loss": 0.0844,
      "step": 410
    },
    {
      "epoch": 5.918727915194347,
      "grad_norm": 0.5842018127441406,
      "learning_rate": 1.6857142857142858e-05,
      "loss": 0.0816,
      "step": 420
    },
    {
      "epoch": 6.056537102473499,
      "grad_norm": 0.48564115166664124,
      "learning_rate": 1.6758620689655175e-05,
      "loss": 0.0763,
      "step": 430
    },
    {
      "epoch": 6.1978798586572434,
      "grad_norm": 0.42375168204307556,
      "learning_rate": 1.666009852216749e-05,
      "loss": 0.0659,
      "step": 440
    },
    {
      "epoch": 6.339222614840989,
      "grad_norm": 0.3496082127094269,
      "learning_rate": 1.6561576354679805e-05,
      "loss": 0.0872,
      "step": 450
    },
    {
      "epoch": 6.480565371024735,
      "grad_norm": 0.3567661941051483,
      "learning_rate": 1.646305418719212e-05,
      "loss": 0.0616,
      "step": 460
    },
    {
      "epoch": 6.6219081272084805,
      "grad_norm": 0.46722063422203064,
      "learning_rate": 1.6364532019704436e-05,
      "loss": 0.0618,
      "step": 470
    },
    {
      "epoch": 6.763250883392226,
      "grad_norm": 0.6222876906394958,
      "learning_rate": 1.626600985221675e-05,
      "loss": 0.065,
      "step": 480
    },
    {
      "epoch": 6.904593639575972,
      "grad_norm": 0.6176102757453918,
      "learning_rate": 1.6167487684729066e-05,
      "loss": 0.0492,
      "step": 490
    },
    {
      "epoch": 7.042402826855124,
      "grad_norm": 0.28196266293525696,
      "learning_rate": 1.6068965517241383e-05,
      "loss": 0.0613,
      "step": 500
    },
    {
      "epoch": 7.18374558303887,
      "grad_norm": 0.4336925446987152,
      "learning_rate": 1.5970443349753693e-05,
      "loss": 0.0476,
      "step": 510
    },
    {
      "epoch": 7.3250883392226145,
      "grad_norm": 0.4347812831401825,
      "learning_rate": 1.587192118226601e-05,
      "loss": 0.0524,
      "step": 520
    },
    {
      "epoch": 7.46643109540636,
      "grad_norm": 0.2817549407482147,
      "learning_rate": 1.5773399014778327e-05,
      "loss": 0.0485,
      "step": 530
    },
    {
      "epoch": 7.607773851590106,
      "grad_norm": 0.43524861335754395,
      "learning_rate": 1.567487684729064e-05,
      "loss": 0.0588,
      "step": 540
    },
    {
      "epoch": 7.749116607773852,
      "grad_norm": 0.38929304480552673,
      "learning_rate": 1.5576354679802958e-05,
      "loss": 0.0527,
      "step": 550
    },
    {
      "epoch": 7.890459363957597,
      "grad_norm": 0.30903270840644836,
      "learning_rate": 1.547783251231527e-05,
      "loss": 0.0437,
      "step": 560
    },
    {
      "epoch": 8.02826855123675,
      "grad_norm": 0.29240888357162476,
      "learning_rate": 1.5379310344827588e-05,
      "loss": 0.0367,
      "step": 570
    },
    {
      "epoch": 8.169611307420494,
      "grad_norm": 0.3683641254901886,
      "learning_rate": 1.5280788177339902e-05,
      "loss": 0.0376,
      "step": 580
    },
    {
      "epoch": 8.31095406360424,
      "grad_norm": 0.6920198798179626,
      "learning_rate": 1.5182266009852219e-05,
      "loss": 0.043,
      "step": 590
    },
    {
      "epoch": 8.452296819787986,
      "grad_norm": 0.5696192383766174,
      "learning_rate": 1.5083743842364534e-05,
      "loss": 0.0368,
      "step": 600
    },
    {
      "epoch": 8.593639575971732,
      "grad_norm": 0.4179733991622925,
      "learning_rate": 1.4985221674876849e-05,
      "loss": 0.0355,
      "step": 610
    },
    {
      "epoch": 8.734982332155477,
      "grad_norm": 0.23820020258426666,
      "learning_rate": 1.4886699507389164e-05,
      "loss": 0.0427,
      "step": 620
    },
    {
      "epoch": 8.876325088339222,
      "grad_norm": 0.29968008399009705,
      "learning_rate": 1.478817733990148e-05,
      "loss": 0.0402,
      "step": 630
    },
    {
      "epoch": 9.014134275618375,
      "grad_norm": 0.3682544529438019,
      "learning_rate": 1.4689655172413795e-05,
      "loss": 0.0298,
      "step": 640
    },
    {
      "epoch": 9.15547703180212,
      "grad_norm": 0.18607290089130402,
      "learning_rate": 1.459113300492611e-05,
      "loss": 0.0288,
      "step": 650
    },
    {
      "epoch": 9.296819787985866,
      "grad_norm": 0.474592000246048,
      "learning_rate": 1.4492610837438425e-05,
      "loss": 0.0286,
      "step": 660
    },
    {
      "epoch": 9.43816254416961,
      "grad_norm": 0.2575773596763611,
      "learning_rate": 1.439408866995074e-05,
      "loss": 0.0261,
      "step": 670
    },
    {
      "epoch": 9.579505300353357,
      "grad_norm": 0.19216832518577576,
      "learning_rate": 1.4295566502463056e-05,
      "loss": 0.0455,
      "step": 680
    },
    {
      "epoch": 9.720848056537102,
      "grad_norm": 0.26925453543663025,
      "learning_rate": 1.4197044334975371e-05,
      "loss": 0.028,
      "step": 690
    },
    {
      "epoch": 9.862190812720849,
      "grad_norm": 0.3649260997772217,
      "learning_rate": 1.4098522167487685e-05,
      "loss": 0.0299,
      "step": 700
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.21814624965190887,
      "learning_rate": 1.4e-05,
      "loss": 0.0377,
      "step": 710
    },
    {
      "epoch": 10.141342756183745,
      "grad_norm": 0.21176187694072723,
      "learning_rate": 1.3901477832512315e-05,
      "loss": 0.0252,
      "step": 720
    },
    {
      "epoch": 10.282685512367491,
      "grad_norm": 0.2577415108680725,
      "learning_rate": 1.380295566502463e-05,
      "loss": 0.0212,
      "step": 730
    },
    {
      "epoch": 10.424028268551236,
      "grad_norm": 0.19473059475421906,
      "learning_rate": 1.3704433497536947e-05,
      "loss": 0.0343,
      "step": 740
    },
    {
      "epoch": 10.565371024734983,
      "grad_norm": 0.2066015899181366,
      "learning_rate": 1.3605911330049262e-05,
      "loss": 0.0364,
      "step": 750
    },
    {
      "epoch": 10.706713780918728,
      "grad_norm": 0.1550784707069397,
      "learning_rate": 1.3507389162561578e-05,
      "loss": 0.034,
      "step": 760
    },
    {
      "epoch": 10.848056537102474,
      "grad_norm": 0.2185942530632019,
      "learning_rate": 1.3408866995073893e-05,
      "loss": 0.0262,
      "step": 770
    },
    {
      "epoch": 10.989399293286219,
      "grad_norm": 0.2593575417995453,
      "learning_rate": 1.3310344827586208e-05,
      "loss": 0.0207,
      "step": 780
    },
    {
      "epoch": 11.12720848056537,
      "grad_norm": 0.19922897219657898,
      "learning_rate": 1.3211822660098523e-05,
      "loss": 0.0195,
      "step": 790
    },
    {
      "epoch": 11.268551236749117,
      "grad_norm": 0.18619875609874725,
      "learning_rate": 1.3113300492610839e-05,
      "loss": 0.0206,
      "step": 800
    },
    {
      "epoch": 11.409893992932862,
      "grad_norm": 0.13659322261810303,
      "learning_rate": 1.3014778325123154e-05,
      "loss": 0.0178,
      "step": 810
    },
    {
      "epoch": 11.551236749116608,
      "grad_norm": 0.22981518507003784,
      "learning_rate": 1.2916256157635469e-05,
      "loss": 0.0269,
      "step": 820
    },
    {
      "epoch": 11.692579505300353,
      "grad_norm": 0.32261478900909424,
      "learning_rate": 1.2817733990147784e-05,
      "loss": 0.0378,
      "step": 830
    },
    {
      "epoch": 11.8339222614841,
      "grad_norm": 0.17600901424884796,
      "learning_rate": 1.27192118226601e-05,
      "loss": 0.0178,
      "step": 840
    },
    {
      "epoch": 11.975265017667844,
      "grad_norm": 0.3149673640727997,
      "learning_rate": 1.2620689655172415e-05,
      "loss": 0.0427,
      "step": 850
    },
    {
      "epoch": 12.113074204946997,
      "grad_norm": 0.18299463391304016,
      "learning_rate": 1.2522167487684732e-05,
      "loss": 0.0252,
      "step": 860
    },
    {
      "epoch": 12.254416961130742,
      "grad_norm": 0.28378018736839294,
      "learning_rate": 1.2423645320197047e-05,
      "loss": 0.028,
      "step": 870
    },
    {
      "epoch": 12.395759717314487,
      "grad_norm": 0.24791936576366425,
      "learning_rate": 1.2325123152709359e-05,
      "loss": 0.0213,
      "step": 880
    },
    {
      "epoch": 12.537102473498233,
      "grad_norm": 0.24097603559494019,
      "learning_rate": 1.2226600985221676e-05,
      "loss": 0.0185,
      "step": 890
    },
    {
      "epoch": 12.678445229681978,
      "grad_norm": 0.25327208638191223,
      "learning_rate": 1.2128078817733991e-05,
      "loss": 0.0227,
      "step": 900
    },
    {
      "epoch": 12.819787985865725,
      "grad_norm": 0.21336989104747772,
      "learning_rate": 1.2029556650246306e-05,
      "loss": 0.037,
      "step": 910
    },
    {
      "epoch": 12.96113074204947,
      "grad_norm": 0.17810556292533875,
      "learning_rate": 1.1931034482758621e-05,
      "loss": 0.0229,
      "step": 920
    },
    {
      "epoch": 13.098939929328623,
      "grad_norm": 0.24144887924194336,
      "learning_rate": 1.1832512315270937e-05,
      "loss": 0.0237,
      "step": 930
    },
    {
      "epoch": 13.240282685512367,
      "grad_norm": 0.3064623177051544,
      "learning_rate": 1.1733990147783252e-05,
      "loss": 0.023,
      "step": 940
    },
    {
      "epoch": 13.381625441696112,
      "grad_norm": 0.349453330039978,
      "learning_rate": 1.1635467980295567e-05,
      "loss": 0.0351,
      "step": 950
    },
    {
      "epoch": 13.522968197879859,
      "grad_norm": 0.1806306689977646,
      "learning_rate": 1.1536945812807882e-05,
      "loss": 0.0163,
      "step": 960
    },
    {
      "epoch": 13.664310954063604,
      "grad_norm": 0.26092809438705444,
      "learning_rate": 1.1438423645320198e-05,
      "loss": 0.026,
      "step": 970
    },
    {
      "epoch": 13.80565371024735,
      "grad_norm": 0.20213349163532257,
      "learning_rate": 1.1339901477832513e-05,
      "loss": 0.0221,
      "step": 980
    },
    {
      "epoch": 13.946996466431095,
      "grad_norm": 0.16191978752613068,
      "learning_rate": 1.1241379310344828e-05,
      "loss": 0.0186,
      "step": 990
    },
    {
      "epoch": 14.084805653710248,
      "grad_norm": 0.2044558823108673,
      "learning_rate": 1.1142857142857143e-05,
      "loss": 0.0207,
      "step": 1000
    },
    {
      "epoch": 14.226148409893993,
      "grad_norm": 0.2999582290649414,
      "learning_rate": 1.104433497536946e-05,
      "loss": 0.017,
      "step": 1010
    },
    {
      "epoch": 14.36749116607774,
      "grad_norm": 0.19729472696781158,
      "learning_rate": 1.0945812807881776e-05,
      "loss": 0.0181,
      "step": 1020
    },
    {
      "epoch": 14.508833922261484,
      "grad_norm": 0.284512996673584,
      "learning_rate": 1.084729064039409e-05,
      "loss": 0.0191,
      "step": 1030
    },
    {
      "epoch": 14.650176678445229,
      "grad_norm": 0.203534334897995,
      "learning_rate": 1.0748768472906406e-05,
      "loss": 0.0239,
      "step": 1040
    },
    {
      "epoch": 14.791519434628976,
      "grad_norm": 0.192623570561409,
      "learning_rate": 1.0650246305418721e-05,
      "loss": 0.0223,
      "step": 1050
    },
    {
      "epoch": 14.93286219081272,
      "grad_norm": 0.20328915119171143,
      "learning_rate": 1.0551724137931037e-05,
      "loss": 0.0367,
      "step": 1060
    },
    {
      "epoch": 15.070671378091873,
      "grad_norm": 0.1692533642053604,
      "learning_rate": 1.045320197044335e-05,
      "loss": 0.018,
      "step": 1070
    },
    {
      "epoch": 15.212014134275618,
      "grad_norm": 0.28852078318595886,
      "learning_rate": 1.0354679802955665e-05,
      "loss": 0.0284,
      "step": 1080
    },
    {
      "epoch": 15.353356890459365,
      "grad_norm": 0.15338106453418732,
      "learning_rate": 1.025615763546798e-05,
      "loss": 0.0214,
      "step": 1090
    },
    {
      "epoch": 15.49469964664311,
      "grad_norm": 0.14837351441383362,
      "learning_rate": 1.0157635467980296e-05,
      "loss": 0.0257,
      "step": 1100
    },
    {
      "epoch": 15.636042402826854,
      "grad_norm": 0.17378336191177368,
      "learning_rate": 1.0059113300492611e-05,
      "loss": 0.0144,
      "step": 1110
    },
    {
      "epoch": 15.777385159010601,
      "grad_norm": 0.1813267022371292,
      "learning_rate": 9.960591133004926e-06,
      "loss": 0.027,
      "step": 1120
    },
    {
      "epoch": 15.918727915194346,
      "grad_norm": 0.17069673538208008,
      "learning_rate": 9.862068965517241e-06,
      "loss": 0.0139,
      "step": 1130
    },
    {
      "epoch": 16.0565371024735,
      "grad_norm": 0.2160128951072693,
      "learning_rate": 9.763546798029557e-06,
      "loss": 0.0202,
      "step": 1140
    },
    {
      "epoch": 16.197879858657245,
      "grad_norm": 0.18141208589076996,
      "learning_rate": 9.665024630541872e-06,
      "loss": 0.0251,
      "step": 1150
    },
    {
      "epoch": 16.33922261484099,
      "grad_norm": 0.22773516178131104,
      "learning_rate": 9.566502463054189e-06,
      "loss": 0.014,
      "step": 1160
    },
    {
      "epoch": 16.480565371024735,
      "grad_norm": 0.20091529190540314,
      "learning_rate": 9.467980295566504e-06,
      "loss": 0.0164,
      "step": 1170
    },
    {
      "epoch": 16.62190812720848,
      "grad_norm": 0.14417500793933868,
      "learning_rate": 9.36945812807882e-06,
      "loss": 0.0271,
      "step": 1180
    },
    {
      "epoch": 16.763250883392224,
      "grad_norm": 0.3887133002281189,
      "learning_rate": 9.270935960591135e-06,
      "loss": 0.0164,
      "step": 1190
    },
    {
      "epoch": 16.90459363957597,
      "grad_norm": 0.21144379675388336,
      "learning_rate": 9.172413793103448e-06,
      "loss": 0.0183,
      "step": 1200
    },
    {
      "epoch": 17.042402826855124,
      "grad_norm": 0.20944753289222717,
      "learning_rate": 9.073891625615763e-06,
      "loss": 0.0268,
      "step": 1210
    },
    {
      "epoch": 17.18374558303887,
      "grad_norm": 0.15941666066646576,
      "learning_rate": 8.975369458128079e-06,
      "loss": 0.0211,
      "step": 1220
    },
    {
      "epoch": 17.325088339222614,
      "grad_norm": 0.15022148191928864,
      "learning_rate": 8.876847290640394e-06,
      "loss": 0.0154,
      "step": 1230
    },
    {
      "epoch": 17.46643109540636,
      "grad_norm": 0.19839166104793549,
      "learning_rate": 8.77832512315271e-06,
      "loss": 0.015,
      "step": 1240
    },
    {
      "epoch": 17.607773851590107,
      "grad_norm": 0.20651251077651978,
      "learning_rate": 8.679802955665026e-06,
      "loss": 0.0216,
      "step": 1250
    },
    {
      "epoch": 17.74911660777385,
      "grad_norm": 0.2176295816898346,
      "learning_rate": 8.581280788177341e-06,
      "loss": 0.0165,
      "step": 1260
    },
    {
      "epoch": 17.890459363957596,
      "grad_norm": 0.19485551118850708,
      "learning_rate": 8.482758620689656e-06,
      "loss": 0.0156,
      "step": 1270
    },
    {
      "epoch": 18.02826855123675,
      "grad_norm": 0.13088421523571014,
      "learning_rate": 8.384236453201972e-06,
      "loss": 0.0164,
      "step": 1280
    },
    {
      "epoch": 18.169611307420496,
      "grad_norm": 0.24724619090557098,
      "learning_rate": 8.285714285714287e-06,
      "loss": 0.0213,
      "step": 1290
    },
    {
      "epoch": 18.31095406360424,
      "grad_norm": 0.271317720413208,
      "learning_rate": 8.1871921182266e-06,
      "loss": 0.0189,
      "step": 1300
    },
    {
      "epoch": 18.452296819787986,
      "grad_norm": 0.14633476734161377,
      "learning_rate": 8.088669950738916e-06,
      "loss": 0.0184,
      "step": 1310
    },
    {
      "epoch": 18.593639575971732,
      "grad_norm": 0.14612814784049988,
      "learning_rate": 7.990147783251233e-06,
      "loss": 0.0136,
      "step": 1320
    },
    {
      "epoch": 18.73498233215548,
      "grad_norm": 0.1322396844625473,
      "learning_rate": 7.891625615763548e-06,
      "loss": 0.0238,
      "step": 1330
    },
    {
      "epoch": 18.87632508833922,
      "grad_norm": 0.12669849395751953,
      "learning_rate": 7.793103448275863e-06,
      "loss": 0.0199,
      "step": 1340
    },
    {
      "epoch": 19.014134275618375,
      "grad_norm": 0.20533867180347443,
      "learning_rate": 7.694581280788178e-06,
      "loss": 0.0159,
      "step": 1350
    },
    {
      "epoch": 19.15547703180212,
      "grad_norm": 0.1605178564786911,
      "learning_rate": 7.596059113300494e-06,
      "loss": 0.021,
      "step": 1360
    },
    {
      "epoch": 19.296819787985864,
      "grad_norm": 0.12252944707870483,
      "learning_rate": 7.497536945812809e-06,
      "loss": 0.012,
      "step": 1370
    },
    {
      "epoch": 19.43816254416961,
      "grad_norm": 0.24101655185222626,
      "learning_rate": 7.399014778325124e-06,
      "loss": 0.0247,
      "step": 1380
    },
    {
      "epoch": 19.579505300353357,
      "grad_norm": 0.1337701827287674,
      "learning_rate": 7.3004926108374385e-06,
      "loss": 0.0142,
      "step": 1390
    },
    {
      "epoch": 19.720848056537104,
      "grad_norm": 0.30575674772262573,
      "learning_rate": 7.201970443349754e-06,
      "loss": 0.0198,
      "step": 1400
    },
    {
      "epoch": 19.862190812720847,
      "grad_norm": 0.22058513760566711,
      "learning_rate": 7.103448275862069e-06,
      "loss": 0.0203,
      "step": 1410
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.13275393843650818,
      "learning_rate": 7.004926108374385e-06,
      "loss": 0.0118,
      "step": 1420
    },
    {
      "epoch": 20.141342756183747,
      "grad_norm": 0.15058188140392303,
      "learning_rate": 6.9064039408867e-06,
      "loss": 0.0179,
      "step": 1430
    },
    {
      "epoch": 20.28268551236749,
      "grad_norm": 0.16335678100585938,
      "learning_rate": 6.8078817733990155e-06,
      "loss": 0.0143,
      "step": 1440
    },
    {
      "epoch": 20.424028268551236,
      "grad_norm": 0.15252141654491425,
      "learning_rate": 6.709359605911331e-06,
      "loss": 0.0148,
      "step": 1450
    },
    {
      "epoch": 20.565371024734983,
      "grad_norm": 0.16172365844249725,
      "learning_rate": 6.610837438423646e-06,
      "loss": 0.0142,
      "step": 1460
    },
    {
      "epoch": 20.70671378091873,
      "grad_norm": 0.1594265103340149,
      "learning_rate": 6.512315270935961e-06,
      "loss": 0.028,
      "step": 1470
    },
    {
      "epoch": 20.848056537102472,
      "grad_norm": 0.15374749898910522,
      "learning_rate": 6.413793103448276e-06,
      "loss": 0.015,
      "step": 1480
    },
    {
      "epoch": 20.98939929328622,
      "grad_norm": 0.140727236866951,
      "learning_rate": 6.315270935960591e-06,
      "loss": 0.0152,
      "step": 1490
    },
    {
      "epoch": 21.127208480565372,
      "grad_norm": 0.14978136122226715,
      "learning_rate": 6.216748768472907e-06,
      "loss": 0.0147,
      "step": 1500
    },
    {
      "epoch": 21.268551236749115,
      "grad_norm": 0.11171155422925949,
      "learning_rate": 6.118226600985222e-06,
      "loss": 0.0292,
      "step": 1510
    },
    {
      "epoch": 21.40989399293286,
      "grad_norm": 0.23375917971134186,
      "learning_rate": 6.0197044334975374e-06,
      "loss": 0.0218,
      "step": 1520
    },
    {
      "epoch": 21.551236749116608,
      "grad_norm": 0.13263636827468872,
      "learning_rate": 5.921182266009853e-06,
      "loss": 0.0121,
      "step": 1530
    },
    {
      "epoch": 21.692579505300355,
      "grad_norm": 0.11136385053396225,
      "learning_rate": 5.822660098522168e-06,
      "loss": 0.0134,
      "step": 1540
    },
    {
      "epoch": 21.833922261484098,
      "grad_norm": 0.1214762032032013,
      "learning_rate": 5.724137931034483e-06,
      "loss": 0.0123,
      "step": 1550
    },
    {
      "epoch": 21.975265017667844,
      "grad_norm": 0.13195690512657166,
      "learning_rate": 5.625615763546799e-06,
      "loss": 0.0111,
      "step": 1560
    },
    {
      "epoch": 22.113074204946997,
      "grad_norm": 0.14987438917160034,
      "learning_rate": 5.527093596059114e-06,
      "loss": 0.0142,
      "step": 1570
    },
    {
      "epoch": 22.25441696113074,
      "grad_norm": 0.15764908492565155,
      "learning_rate": 5.428571428571429e-06,
      "loss": 0.016,
      "step": 1580
    },
    {
      "epoch": 22.395759717314487,
      "grad_norm": 0.15881168842315674,
      "learning_rate": 5.330049261083744e-06,
      "loss": 0.0115,
      "step": 1590
    },
    {
      "epoch": 22.537102473498233,
      "grad_norm": 0.1381656974554062,
      "learning_rate": 5.231527093596059e-06,
      "loss": 0.0112,
      "step": 1600
    },
    {
      "epoch": 22.67844522968198,
      "grad_norm": 0.10949940979480743,
      "learning_rate": 5.133004926108375e-06,
      "loss": 0.0138,
      "step": 1610
    },
    {
      "epoch": 22.819787985865723,
      "grad_norm": 0.3002312183380127,
      "learning_rate": 5.03448275862069e-06,
      "loss": 0.0255,
      "step": 1620
    },
    {
      "epoch": 22.96113074204947,
      "grad_norm": 0.11727957427501678,
      "learning_rate": 4.935960591133006e-06,
      "loss": 0.0188,
      "step": 1630
    },
    {
      "epoch": 23.098939929328623,
      "grad_norm": 0.15172576904296875,
      "learning_rate": 4.83743842364532e-06,
      "loss": 0.0136,
      "step": 1640
    },
    {
      "epoch": 23.24028268551237,
      "grad_norm": 0.1338053196668625,
      "learning_rate": 4.7389162561576355e-06,
      "loss": 0.0226,
      "step": 1650
    },
    {
      "epoch": 23.381625441696112,
      "grad_norm": 0.13890604674816132,
      "learning_rate": 4.640394088669951e-06,
      "loss": 0.0131,
      "step": 1660
    },
    {
      "epoch": 23.52296819787986,
      "grad_norm": 0.4221065044403076,
      "learning_rate": 4.541871921182267e-06,
      "loss": 0.0203,
      "step": 1670
    },
    {
      "epoch": 23.664310954063605,
      "grad_norm": 0.1802377700805664,
      "learning_rate": 4.443349753694582e-06,
      "loss": 0.0177,
      "step": 1680
    },
    {
      "epoch": 23.80565371024735,
      "grad_norm": 0.15613073110580444,
      "learning_rate": 4.3448275862068965e-06,
      "loss": 0.0115,
      "step": 1690
    },
    {
      "epoch": 23.946996466431095,
      "grad_norm": 0.20417454838752747,
      "learning_rate": 4.246305418719212e-06,
      "loss": 0.0126,
      "step": 1700
    },
    {
      "epoch": 24.084805653710248,
      "grad_norm": 0.39553001523017883,
      "learning_rate": 4.147783251231528e-06,
      "loss": 0.0184,
      "step": 1710
    },
    {
      "epoch": 24.226148409893995,
      "grad_norm": 0.16783495247364044,
      "learning_rate": 4.049261083743843e-06,
      "loss": 0.0201,
      "step": 1720
    },
    {
      "epoch": 24.367491166077738,
      "grad_norm": 0.12519846856594086,
      "learning_rate": 3.9507389162561574e-06,
      "loss": 0.0143,
      "step": 1730
    },
    {
      "epoch": 24.508833922261484,
      "grad_norm": 0.1804841309785843,
      "learning_rate": 3.852216748768473e-06,
      "loss": 0.0127,
      "step": 1740
    },
    {
      "epoch": 24.65017667844523,
      "grad_norm": 0.18236030638217926,
      "learning_rate": 3.7536945812807883e-06,
      "loss": 0.0129,
      "step": 1750
    },
    {
      "epoch": 24.791519434628974,
      "grad_norm": 0.15485695004463196,
      "learning_rate": 3.655172413793104e-06,
      "loss": 0.0167,
      "step": 1760
    },
    {
      "epoch": 24.93286219081272,
      "grad_norm": 0.19893868267536163,
      "learning_rate": 3.5566502463054192e-06,
      "loss": 0.0138,
      "step": 1770
    },
    {
      "epoch": 25.070671378091873,
      "grad_norm": 0.2409440129995346,
      "learning_rate": 3.458128078817734e-06,
      "loss": 0.0155,
      "step": 1780
    },
    {
      "epoch": 25.21201413427562,
      "grad_norm": 0.2992894947528839,
      "learning_rate": 3.3596059113300493e-06,
      "loss": 0.0137,
      "step": 1790
    },
    {
      "epoch": 25.353356890459363,
      "grad_norm": 0.21317654848098755,
      "learning_rate": 3.261083743842365e-06,
      "loss": 0.0118,
      "step": 1800
    },
    {
      "epoch": 25.49469964664311,
      "grad_norm": 0.09024431556463242,
      "learning_rate": 3.16256157635468e-06,
      "loss": 0.0123,
      "step": 1810
    },
    {
      "epoch": 25.636042402826856,
      "grad_norm": 0.16717907786369324,
      "learning_rate": 3.0640394088669954e-06,
      "loss": 0.0107,
      "step": 1820
    },
    {
      "epoch": 25.7773851590106,
      "grad_norm": 0.09854767471551895,
      "learning_rate": 2.9655172413793102e-06,
      "loss": 0.0103,
      "step": 1830
    },
    {
      "epoch": 25.918727915194346,
      "grad_norm": 0.21805976331233978,
      "learning_rate": 2.866995073891626e-06,
      "loss": 0.0147,
      "step": 1840
    },
    {
      "epoch": 26.0565371024735,
      "grad_norm": 0.20243337750434875,
      "learning_rate": 2.768472906403941e-06,
      "loss": 0.0225,
      "step": 1850
    },
    {
      "epoch": 26.197879858657245,
      "grad_norm": 0.19805754721164703,
      "learning_rate": 2.6699507389162564e-06,
      "loss": 0.0224,
      "step": 1860
    },
    {
      "epoch": 26.33922261484099,
      "grad_norm": 0.2010664939880371,
      "learning_rate": 2.571428571428571e-06,
      "loss": 0.0148,
      "step": 1870
    },
    {
      "epoch": 26.480565371024735,
      "grad_norm": 0.14873865246772766,
      "learning_rate": 2.472906403940887e-06,
      "loss": 0.0121,
      "step": 1880
    },
    {
      "epoch": 26.62190812720848,
      "grad_norm": 0.137409508228302,
      "learning_rate": 2.374384236453202e-06,
      "loss": 0.012,
      "step": 1890
    },
    {
      "epoch": 26.763250883392224,
      "grad_norm": 0.35608431696891785,
      "learning_rate": 2.2758620689655173e-06,
      "loss": 0.0161,
      "step": 1900
    },
    {
      "epoch": 26.90459363957597,
      "grad_norm": 0.10985977947711945,
      "learning_rate": 2.1773399014778326e-06,
      "loss": 0.0104,
      "step": 1910
    },
    {
      "epoch": 27.042402826855124,
      "grad_norm": 0.14658188819885254,
      "learning_rate": 2.0788177339901482e-06,
      "loss": 0.0115,
      "step": 1920
    },
    {
      "epoch": 27.18374558303887,
      "grad_norm": 0.26340097188949585,
      "learning_rate": 1.980295566502463e-06,
      "loss": 0.0117,
      "step": 1930
    },
    {
      "epoch": 27.325088339222614,
      "grad_norm": 0.13159722089767456,
      "learning_rate": 1.8817733990147785e-06,
      "loss": 0.0122,
      "step": 1940
    },
    {
      "epoch": 27.46643109540636,
      "grad_norm": 0.11752301454544067,
      "learning_rate": 1.7832512315270935e-06,
      "loss": 0.0172,
      "step": 1950
    },
    {
      "epoch": 27.607773851590107,
      "grad_norm": 0.16132904589176178,
      "learning_rate": 1.684729064039409e-06,
      "loss": 0.0105,
      "step": 1960
    },
    {
      "epoch": 27.74911660777385,
      "grad_norm": 0.13001614809036255,
      "learning_rate": 1.5862068965517244e-06,
      "loss": 0.0105,
      "step": 1970
    },
    {
      "epoch": 27.890459363957596,
      "grad_norm": 0.14783978462219238,
      "learning_rate": 1.4876847290640394e-06,
      "loss": 0.0158,
      "step": 1980
    },
    {
      "epoch": 28.02826855123675,
      "grad_norm": 0.1387936919927597,
      "learning_rate": 1.3891625615763549e-06,
      "loss": 0.0172,
      "step": 1990
    },
    {
      "epoch": 28.169611307420496,
      "grad_norm": 0.14778320491313934,
      "learning_rate": 1.29064039408867e-06,
      "loss": 0.0135,
      "step": 2000
    },
    {
      "epoch": 28.31095406360424,
      "grad_norm": 0.09274590760469437,
      "learning_rate": 1.1921182266009854e-06,
      "loss": 0.0102,
      "step": 2010
    },
    {
      "epoch": 28.452296819787986,
      "grad_norm": 0.18421560525894165,
      "learning_rate": 1.0935960591133006e-06,
      "loss": 0.0227,
      "step": 2020
    },
    {
      "epoch": 28.593639575971732,
      "grad_norm": 0.15715081989765167,
      "learning_rate": 9.950738916256158e-07,
      "loss": 0.0106,
      "step": 2030
    },
    {
      "epoch": 28.73498233215548,
      "grad_norm": 0.1885443925857544,
      "learning_rate": 8.965517241379311e-07,
      "loss": 0.0122,
      "step": 2040
    },
    {
      "epoch": 28.87632508833922,
      "grad_norm": 0.1896154135465622,
      "learning_rate": 7.980295566502463e-07,
      "loss": 0.0164,
      "step": 2050
    },
    {
      "epoch": 29.014134275618375,
      "grad_norm": 0.09104613959789276,
      "learning_rate": 6.995073891625616e-07,
      "loss": 0.0119,
      "step": 2060
    },
    {
      "epoch": 29.15547703180212,
      "grad_norm": 0.08983226120471954,
      "learning_rate": 6.009852216748769e-07,
      "loss": 0.0127,
      "step": 2070
    },
    {
      "epoch": 29.296819787985864,
      "grad_norm": 0.1671595722436905,
      "learning_rate": 5.024630541871921e-07,
      "loss": 0.0137,
      "step": 2080
    },
    {
      "epoch": 29.43816254416961,
      "grad_norm": 0.13711874186992645,
      "learning_rate": 4.039408866995075e-07,
      "loss": 0.0105,
      "step": 2090
    },
    {
      "epoch": 29.579505300353357,
      "grad_norm": 0.10631149262189865,
      "learning_rate": 3.054187192118227e-07,
      "loss": 0.0222,
      "step": 2100
    },
    {
      "epoch": 29.720848056537104,
      "grad_norm": 0.12938536703586578,
      "learning_rate": 2.0689655172413796e-07,
      "loss": 0.0102,
      "step": 2110
    },
    {
      "epoch": 29.862190812720847,
      "grad_norm": 0.11636127531528473,
      "learning_rate": 1.0837438423645321e-07,
      "loss": 0.011,
      "step": 2120
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.19212181866168976,
      "learning_rate": 9.852216748768474e-09,
      "loss": 0.0156,
      "step": 2130
    }
  ],
  "logging_steps": 10,
  "max_steps": 2130,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 30,
  "save_steps": 5,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.399812888071615e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
