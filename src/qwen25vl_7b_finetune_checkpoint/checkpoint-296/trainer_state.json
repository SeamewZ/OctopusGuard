{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 296,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02735042735042735,
      "grad_norm": 7.987917423248291,
      "learning_rate": 0.0,
      "loss": 1.4813,
      "step": 1
    },
    {
      "epoch": 0.0547008547008547,
      "grad_norm": 7.66396427154541,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 1.4442,
      "step": 2
    },
    {
      "epoch": 0.08205128205128205,
      "grad_norm": 7.644695281982422,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 1.4567,
      "step": 3
    },
    {
      "epoch": 0.1094017094017094,
      "grad_norm": 7.0296950340271,
      "learning_rate": 6.000000000000001e-07,
      "loss": 1.4358,
      "step": 4
    },
    {
      "epoch": 0.13675213675213677,
      "grad_norm": 7.761176109313965,
      "learning_rate": 8.000000000000001e-07,
      "loss": 1.4034,
      "step": 5
    },
    {
      "epoch": 0.1641025641025641,
      "grad_norm": 7.808496952056885,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.5038,
      "step": 6
    },
    {
      "epoch": 0.19145299145299147,
      "grad_norm": 7.603172302246094,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 1.5054,
      "step": 7
    },
    {
      "epoch": 0.2188034188034188,
      "grad_norm": 7.7549729347229,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 1.4621,
      "step": 8
    },
    {
      "epoch": 0.24615384615384617,
      "grad_norm": 7.926301956176758,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 1.5055,
      "step": 9
    },
    {
      "epoch": 0.27350427350427353,
      "grad_norm": 8.063470840454102,
      "learning_rate": 1.8000000000000001e-06,
      "loss": 1.4444,
      "step": 10
    },
    {
      "epoch": 0.30085470085470084,
      "grad_norm": 7.68711519241333,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.3568,
      "step": 11
    },
    {
      "epoch": 0.3282051282051282,
      "grad_norm": 7.831308364868164,
      "learning_rate": 2.2e-06,
      "loss": 1.4202,
      "step": 12
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 8.275554656982422,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 1.4877,
      "step": 13
    },
    {
      "epoch": 0.38290598290598293,
      "grad_norm": 7.794364929199219,
      "learning_rate": 2.6e-06,
      "loss": 1.4109,
      "step": 14
    },
    {
      "epoch": 0.41025641025641024,
      "grad_norm": 7.878113746643066,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 1.4023,
      "step": 15
    },
    {
      "epoch": 0.4376068376068376,
      "grad_norm": 7.82016134262085,
      "learning_rate": 3e-06,
      "loss": 1.4253,
      "step": 16
    },
    {
      "epoch": 0.46495726495726497,
      "grad_norm": 7.576168060302734,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 1.4016,
      "step": 17
    },
    {
      "epoch": 0.49230769230769234,
      "grad_norm": 8.070490837097168,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 1.4653,
      "step": 18
    },
    {
      "epoch": 0.5196581196581197,
      "grad_norm": 8.03287410736084,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 1.329,
      "step": 19
    },
    {
      "epoch": 0.5470085470085471,
      "grad_norm": 7.967064380645752,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 1.3726,
      "step": 20
    },
    {
      "epoch": 0.5743589743589743,
      "grad_norm": 8.15339469909668,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.3717,
      "step": 21
    },
    {
      "epoch": 0.6017094017094017,
      "grad_norm": 7.402429103851318,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 1.274,
      "step": 22
    },
    {
      "epoch": 0.629059829059829,
      "grad_norm": 7.548104763031006,
      "learning_rate": 4.4e-06,
      "loss": 1.3269,
      "step": 23
    },
    {
      "epoch": 0.6564102564102564,
      "grad_norm": 7.525612831115723,
      "learning_rate": 4.600000000000001e-06,
      "loss": 1.2921,
      "step": 24
    },
    {
      "epoch": 0.6837606837606838,
      "grad_norm": 7.651641368865967,
      "learning_rate": 4.800000000000001e-06,
      "loss": 1.3203,
      "step": 25
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 6.718221187591553,
      "learning_rate": 5e-06,
      "loss": 1.2635,
      "step": 26
    },
    {
      "epoch": 0.7384615384615385,
      "grad_norm": 7.071441650390625,
      "learning_rate": 5.2e-06,
      "loss": 1.2072,
      "step": 27
    },
    {
      "epoch": 0.7658119658119659,
      "grad_norm": 6.649914741516113,
      "learning_rate": 5.400000000000001e-06,
      "loss": 1.1123,
      "step": 28
    },
    {
      "epoch": 0.7931623931623931,
      "grad_norm": 6.471934795379639,
      "learning_rate": 5.600000000000001e-06,
      "loss": 1.1359,
      "step": 29
    },
    {
      "epoch": 0.8205128205128205,
      "grad_norm": 5.86865758895874,
      "learning_rate": 5.8e-06,
      "loss": 1.0135,
      "step": 30
    },
    {
      "epoch": 0.8478632478632478,
      "grad_norm": 5.500441551208496,
      "learning_rate": 6e-06,
      "loss": 0.9778,
      "step": 31
    },
    {
      "epoch": 0.8752136752136752,
      "grad_norm": 5.167566299438477,
      "learning_rate": 6.200000000000001e-06,
      "loss": 0.9648,
      "step": 32
    },
    {
      "epoch": 0.9025641025641026,
      "grad_norm": 4.539048671722412,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.8926,
      "step": 33
    },
    {
      "epoch": 0.9299145299145299,
      "grad_norm": 4.262248516082764,
      "learning_rate": 6.600000000000001e-06,
      "loss": 0.8558,
      "step": 34
    },
    {
      "epoch": 0.9572649572649573,
      "grad_norm": 3.4421210289001465,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.7261,
      "step": 35
    },
    {
      "epoch": 0.9846153846153847,
      "grad_norm": 3.0223870277404785,
      "learning_rate": 7e-06,
      "loss": 0.739,
      "step": 36
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.5930604934692383,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 0.719,
      "step": 37
    },
    {
      "epoch": 1.0273504273504273,
      "grad_norm": 2.0177149772644043,
      "learning_rate": 7.4e-06,
      "loss": 0.6388,
      "step": 38
    },
    {
      "epoch": 1.0547008547008547,
      "grad_norm": 1.5234456062316895,
      "learning_rate": 7.600000000000001e-06,
      "loss": 0.5719,
      "step": 39
    },
    {
      "epoch": 1.082051282051282,
      "grad_norm": 1.3531112670898438,
      "learning_rate": 7.800000000000002e-06,
      "loss": 0.6468,
      "step": 40
    },
    {
      "epoch": 1.1094017094017095,
      "grad_norm": 1.0982097387313843,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.57,
      "step": 41
    },
    {
      "epoch": 1.1367521367521367,
      "grad_norm": 0.8852293491363525,
      "learning_rate": 8.2e-06,
      "loss": 0.5413,
      "step": 42
    },
    {
      "epoch": 1.1641025641025642,
      "grad_norm": 0.7836958765983582,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.5827,
      "step": 43
    },
    {
      "epoch": 1.1914529914529914,
      "grad_norm": 0.5614312291145325,
      "learning_rate": 8.6e-06,
      "loss": 0.4436,
      "step": 44
    },
    {
      "epoch": 1.218803418803419,
      "grad_norm": 0.6244640946388245,
      "learning_rate": 8.8e-06,
      "loss": 0.5331,
      "step": 45
    },
    {
      "epoch": 1.2461538461538462,
      "grad_norm": 0.5135221481323242,
      "learning_rate": 9e-06,
      "loss": 0.5006,
      "step": 46
    },
    {
      "epoch": 1.2735042735042734,
      "grad_norm": 0.45969489216804504,
      "learning_rate": 9.200000000000002e-06,
      "loss": 0.4606,
      "step": 47
    },
    {
      "epoch": 1.300854700854701,
      "grad_norm": 0.4803638160228729,
      "learning_rate": 9.4e-06,
      "loss": 0.5068,
      "step": 48
    },
    {
      "epoch": 1.3282051282051281,
      "grad_norm": 0.4018401503562927,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.4729,
      "step": 49
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.40501710772514343,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.4568,
      "step": 50
    },
    {
      "epoch": 1.3829059829059829,
      "grad_norm": 0.3697502911090851,
      "learning_rate": 1e-05,
      "loss": 0.4941,
      "step": 51
    },
    {
      "epoch": 1.4102564102564101,
      "grad_norm": 0.33122244477272034,
      "learning_rate": 1.02e-05,
      "loss": 0.4337,
      "step": 52
    },
    {
      "epoch": 1.4376068376068376,
      "grad_norm": 0.325345516204834,
      "learning_rate": 1.04e-05,
      "loss": 0.4736,
      "step": 53
    },
    {
      "epoch": 1.464957264957265,
      "grad_norm": 0.2586267590522766,
      "learning_rate": 1.0600000000000002e-05,
      "loss": 0.3963,
      "step": 54
    },
    {
      "epoch": 1.4923076923076923,
      "grad_norm": 0.27050891518592834,
      "learning_rate": 1.0800000000000002e-05,
      "loss": 0.4068,
      "step": 55
    },
    {
      "epoch": 1.5196581196581196,
      "grad_norm": 0.2550108730792999,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.4229,
      "step": 56
    },
    {
      "epoch": 1.547008547008547,
      "grad_norm": 0.2712404131889343,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.4107,
      "step": 57
    },
    {
      "epoch": 1.5743589743589743,
      "grad_norm": 0.2664663791656494,
      "learning_rate": 1.14e-05,
      "loss": 0.4605,
      "step": 58
    },
    {
      "epoch": 1.6017094017094018,
      "grad_norm": 0.25246354937553406,
      "learning_rate": 1.16e-05,
      "loss": 0.3888,
      "step": 59
    },
    {
      "epoch": 1.629059829059829,
      "grad_norm": 0.2377675473690033,
      "learning_rate": 1.18e-05,
      "loss": 0.4057,
      "step": 60
    },
    {
      "epoch": 1.6564102564102563,
      "grad_norm": 0.23129336535930634,
      "learning_rate": 1.2e-05,
      "loss": 0.3816,
      "step": 61
    },
    {
      "epoch": 1.6837606837606838,
      "grad_norm": 0.22450298070907593,
      "learning_rate": 1.22e-05,
      "loss": 0.3898,
      "step": 62
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.22516503930091858,
      "learning_rate": 1.2400000000000002e-05,
      "loss": 0.382,
      "step": 63
    },
    {
      "epoch": 1.7384615384615385,
      "grad_norm": 0.2264064997434616,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.378,
      "step": 64
    },
    {
      "epoch": 1.7658119658119658,
      "grad_norm": 0.2158059924840927,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.405,
      "step": 65
    },
    {
      "epoch": 1.793162393162393,
      "grad_norm": 0.21801210939884186,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.4419,
      "step": 66
    },
    {
      "epoch": 1.8205128205128205,
      "grad_norm": 0.1957392543554306,
      "learning_rate": 1.3200000000000002e-05,
      "loss": 0.3815,
      "step": 67
    },
    {
      "epoch": 1.847863247863248,
      "grad_norm": 0.18614785373210907,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.3217,
      "step": 68
    },
    {
      "epoch": 1.8752136752136752,
      "grad_norm": 0.17699186503887177,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.3825,
      "step": 69
    },
    {
      "epoch": 1.9025641025641025,
      "grad_norm": 0.1707170158624649,
      "learning_rate": 1.38e-05,
      "loss": 0.3249,
      "step": 70
    },
    {
      "epoch": 1.92991452991453,
      "grad_norm": 0.18163268268108368,
      "learning_rate": 1.4e-05,
      "loss": 0.3864,
      "step": 71
    },
    {
      "epoch": 1.9572649572649574,
      "grad_norm": 0.18788853287696838,
      "learning_rate": 1.4200000000000001e-05,
      "loss": 0.3911,
      "step": 72
    },
    {
      "epoch": 1.9846153846153847,
      "grad_norm": 0.15869013965129852,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 0.3035,
      "step": 73
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.1680258810520172,
      "learning_rate": 1.46e-05,
      "loss": 0.3669,
      "step": 74
    },
    {
      "epoch": 2.0273504273504273,
      "grad_norm": 0.16483145952224731,
      "learning_rate": 1.48e-05,
      "loss": 0.3473,
      "step": 75
    },
    {
      "epoch": 2.0547008547008545,
      "grad_norm": 0.15983140468597412,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 0.3523,
      "step": 76
    },
    {
      "epoch": 2.082051282051282,
      "grad_norm": 0.15423761308193207,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 0.3202,
      "step": 77
    },
    {
      "epoch": 2.1094017094017095,
      "grad_norm": 0.15952742099761963,
      "learning_rate": 1.54e-05,
      "loss": 0.3132,
      "step": 78
    },
    {
      "epoch": 2.1367521367521367,
      "grad_norm": 0.15116187930107117,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 0.3442,
      "step": 79
    },
    {
      "epoch": 2.164102564102564,
      "grad_norm": 0.14677153527736664,
      "learning_rate": 1.58e-05,
      "loss": 0.3379,
      "step": 80
    },
    {
      "epoch": 2.1914529914529917,
      "grad_norm": 0.14955748617649078,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.3085,
      "step": 81
    },
    {
      "epoch": 2.218803418803419,
      "grad_norm": 0.14871904253959656,
      "learning_rate": 1.62e-05,
      "loss": 0.2996,
      "step": 82
    },
    {
      "epoch": 2.246153846153846,
      "grad_norm": 0.15230198204517365,
      "learning_rate": 1.64e-05,
      "loss": 0.3187,
      "step": 83
    },
    {
      "epoch": 2.2735042735042734,
      "grad_norm": 0.14402177929878235,
      "learning_rate": 1.66e-05,
      "loss": 0.2868,
      "step": 84
    },
    {
      "epoch": 2.3008547008547007,
      "grad_norm": 0.14254513382911682,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.2748,
      "step": 85
    },
    {
      "epoch": 2.3282051282051284,
      "grad_norm": 0.14636437594890594,
      "learning_rate": 1.7e-05,
      "loss": 0.3244,
      "step": 86
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 0.13683845102787018,
      "learning_rate": 1.72e-05,
      "loss": 0.2733,
      "step": 87
    },
    {
      "epoch": 2.382905982905983,
      "grad_norm": 0.142297625541687,
      "learning_rate": 1.7400000000000003e-05,
      "loss": 0.2606,
      "step": 88
    },
    {
      "epoch": 2.41025641025641,
      "grad_norm": 0.14815042912960052,
      "learning_rate": 1.76e-05,
      "loss": 0.3011,
      "step": 89
    },
    {
      "epoch": 2.437606837606838,
      "grad_norm": 0.14755573868751526,
      "learning_rate": 1.7800000000000002e-05,
      "loss": 0.303,
      "step": 90
    },
    {
      "epoch": 2.464957264957265,
      "grad_norm": 0.1441686749458313,
      "learning_rate": 1.8e-05,
      "loss": 0.2909,
      "step": 91
    },
    {
      "epoch": 2.4923076923076923,
      "grad_norm": 0.1412302702665329,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.286,
      "step": 92
    },
    {
      "epoch": 2.5196581196581196,
      "grad_norm": 0.14406071603298187,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 0.2838,
      "step": 93
    },
    {
      "epoch": 2.547008547008547,
      "grad_norm": 0.1490824818611145,
      "learning_rate": 1.86e-05,
      "loss": 0.2511,
      "step": 94
    },
    {
      "epoch": 2.574358974358974,
      "grad_norm": 0.15443278849124908,
      "learning_rate": 1.88e-05,
      "loss": 0.3375,
      "step": 95
    },
    {
      "epoch": 2.601709401709402,
      "grad_norm": 0.1506425440311432,
      "learning_rate": 1.9e-05,
      "loss": 0.2704,
      "step": 96
    },
    {
      "epoch": 2.629059829059829,
      "grad_norm": 0.14462308585643768,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.2427,
      "step": 97
    },
    {
      "epoch": 2.6564102564102563,
      "grad_norm": 0.14156214892864227,
      "learning_rate": 1.94e-05,
      "loss": 0.3062,
      "step": 98
    },
    {
      "epoch": 2.683760683760684,
      "grad_norm": 0.1278909593820572,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.2193,
      "step": 99
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 0.13522380590438843,
      "learning_rate": 1.98e-05,
      "loss": 0.2876,
      "step": 100
    },
    {
      "epoch": 2.7384615384615385,
      "grad_norm": 0.12907426059246063,
      "learning_rate": 2e-05,
      "loss": 0.2614,
      "step": 101
    },
    {
      "epoch": 2.7658119658119658,
      "grad_norm": 0.1195090264081955,
      "learning_rate": 1.9999761633493754e-05,
      "loss": 0.2389,
      "step": 102
    },
    {
      "epoch": 2.793162393162393,
      "grad_norm": 0.11437439173460007,
      "learning_rate": 1.999904654533872e-05,
      "loss": 0.2537,
      "step": 103
    },
    {
      "epoch": 2.8205128205128203,
      "grad_norm": 0.111420176923275,
      "learning_rate": 1.999785476962552e-05,
      "loss": 0.2318,
      "step": 104
    },
    {
      "epoch": 2.847863247863248,
      "grad_norm": 0.09969443827867508,
      "learning_rate": 1.9996186363170037e-05,
      "loss": 0.2492,
      "step": 105
    },
    {
      "epoch": 2.875213675213675,
      "grad_norm": 0.09806439280509949,
      "learning_rate": 1.9994041405510705e-05,
      "loss": 0.309,
      "step": 106
    },
    {
      "epoch": 2.9025641025641025,
      "grad_norm": 0.09152746945619583,
      "learning_rate": 1.999141999890475e-05,
      "loss": 0.2324,
      "step": 107
    },
    {
      "epoch": 2.92991452991453,
      "grad_norm": 0.08643756806850433,
      "learning_rate": 1.998832226832327e-05,
      "loss": 0.2516,
      "step": 108
    },
    {
      "epoch": 2.9572649572649574,
      "grad_norm": 0.09218280762434006,
      "learning_rate": 1.9984748361445306e-05,
      "loss": 0.2567,
      "step": 109
    },
    {
      "epoch": 2.9846153846153847,
      "grad_norm": 0.0920925885438919,
      "learning_rate": 1.9980698448650805e-05,
      "loss": 0.2236,
      "step": 110
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.08888091146945953,
      "learning_rate": 1.997617272301248e-05,
      "loss": 0.2482,
      "step": 111
    },
    {
      "epoch": 3.0273504273504273,
      "grad_norm": 0.09017451852560043,
      "learning_rate": 1.9971171400286602e-05,
      "loss": 0.2187,
      "step": 112
    },
    {
      "epoch": 3.0547008547008545,
      "grad_norm": 0.09539121389389038,
      "learning_rate": 1.9965694718902745e-05,
      "loss": 0.232,
      "step": 113
    },
    {
      "epoch": 3.082051282051282,
      "grad_norm": 0.0977785512804985,
      "learning_rate": 1.9959742939952393e-05,
      "loss": 0.2567,
      "step": 114
    },
    {
      "epoch": 3.1094017094017095,
      "grad_norm": 0.08931530267000198,
      "learning_rate": 1.995331634717649e-05,
      "loss": 0.2455,
      "step": 115
    },
    {
      "epoch": 3.1367521367521367,
      "grad_norm": 0.09710542112588882,
      "learning_rate": 1.9946415246951928e-05,
      "loss": 0.2622,
      "step": 116
    },
    {
      "epoch": 3.164102564102564,
      "grad_norm": 0.09166442602872849,
      "learning_rate": 1.9939039968276942e-05,
      "loss": 0.24,
      "step": 117
    },
    {
      "epoch": 3.1914529914529917,
      "grad_norm": 0.08506960421800613,
      "learning_rate": 1.9931190862755416e-05,
      "loss": 0.2254,
      "step": 118
    },
    {
      "epoch": 3.218803418803419,
      "grad_norm": 0.08662322163581848,
      "learning_rate": 1.992286830458012e-05,
      "loss": 0.2128,
      "step": 119
    },
    {
      "epoch": 3.246153846153846,
      "grad_norm": 0.08102051168680191,
      "learning_rate": 1.991407269051487e-05,
      "loss": 0.2383,
      "step": 120
    },
    {
      "epoch": 3.2735042735042734,
      "grad_norm": 0.08581721037626266,
      "learning_rate": 1.9904804439875635e-05,
      "loss": 0.2131,
      "step": 121
    },
    {
      "epoch": 3.3008547008547007,
      "grad_norm": 0.06593569368124008,
      "learning_rate": 1.9895063994510512e-05,
      "loss": 0.1799,
      "step": 122
    },
    {
      "epoch": 3.3282051282051284,
      "grad_norm": 0.08600173890590668,
      "learning_rate": 1.9884851818778695e-05,
      "loss": 0.2495,
      "step": 123
    },
    {
      "epoch": 3.3555555555555556,
      "grad_norm": 0.09221372753381729,
      "learning_rate": 1.9874168399528307e-05,
      "loss": 0.2458,
      "step": 124
    },
    {
      "epoch": 3.382905982905983,
      "grad_norm": 0.10095776617527008,
      "learning_rate": 1.9863014246073216e-05,
      "loss": 0.2583,
      "step": 125
    },
    {
      "epoch": 3.41025641025641,
      "grad_norm": 0.08664434403181076,
      "learning_rate": 1.9851389890168738e-05,
      "loss": 0.2262,
      "step": 126
    },
    {
      "epoch": 3.437606837606838,
      "grad_norm": 0.08737509697675705,
      "learning_rate": 1.98392958859863e-05,
      "loss": 0.1846,
      "step": 127
    },
    {
      "epoch": 3.464957264957265,
      "grad_norm": 0.08636811375617981,
      "learning_rate": 1.9826732810087e-05,
      "loss": 0.1773,
      "step": 128
    },
    {
      "epoch": 3.4923076923076923,
      "grad_norm": 0.0885382667183876,
      "learning_rate": 1.9813701261394136e-05,
      "loss": 0.1858,
      "step": 129
    },
    {
      "epoch": 3.5196581196581196,
      "grad_norm": 0.07889077812433243,
      "learning_rate": 1.9800201861164665e-05,
      "loss": 0.2265,
      "step": 130
    },
    {
      "epoch": 3.547008547008547,
      "grad_norm": 0.075199656188488,
      "learning_rate": 1.9786235252959555e-05,
      "loss": 0.2033,
      "step": 131
    },
    {
      "epoch": 3.574358974358974,
      "grad_norm": 0.09402148425579071,
      "learning_rate": 1.9771802102613127e-05,
      "loss": 0.1782,
      "step": 132
    },
    {
      "epoch": 3.601709401709402,
      "grad_norm": 0.0896390900015831,
      "learning_rate": 1.975690309820131e-05,
      "loss": 0.2261,
      "step": 133
    },
    {
      "epoch": 3.629059829059829,
      "grad_norm": 0.07951242476701736,
      "learning_rate": 1.9741538950008817e-05,
      "loss": 0.2219,
      "step": 134
    },
    {
      "epoch": 3.6564102564102563,
      "grad_norm": 0.0731760784983635,
      "learning_rate": 1.972571039049533e-05,
      "loss": 0.1752,
      "step": 135
    },
    {
      "epoch": 3.683760683760684,
      "grad_norm": 0.07174649089574814,
      "learning_rate": 1.9709418174260523e-05,
      "loss": 0.1567,
      "step": 136
    },
    {
      "epoch": 3.7111111111111112,
      "grad_norm": 0.07180537283420563,
      "learning_rate": 1.969266307800813e-05,
      "loss": 0.1592,
      "step": 137
    },
    {
      "epoch": 3.7384615384615385,
      "grad_norm": 0.08598094433546066,
      "learning_rate": 1.967544590050891e-05,
      "loss": 0.2154,
      "step": 138
    },
    {
      "epoch": 3.7658119658119658,
      "grad_norm": 0.07512854784727097,
      "learning_rate": 1.9657767462562544e-05,
      "loss": 0.1576,
      "step": 139
    },
    {
      "epoch": 3.793162393162393,
      "grad_norm": 0.08845619112253189,
      "learning_rate": 1.9639628606958535e-05,
      "loss": 0.2307,
      "step": 140
    },
    {
      "epoch": 3.8205128205128203,
      "grad_norm": 0.06778135895729065,
      "learning_rate": 1.9621030198436007e-05,
      "loss": 0.1696,
      "step": 141
    },
    {
      "epoch": 3.847863247863248,
      "grad_norm": 0.08210355788469315,
      "learning_rate": 1.9601973123642493e-05,
      "loss": 0.2141,
      "step": 142
    },
    {
      "epoch": 3.875213675213675,
      "grad_norm": 0.08244403451681137,
      "learning_rate": 1.9582458291091664e-05,
      "loss": 0.1743,
      "step": 143
    },
    {
      "epoch": 3.9025641025641025,
      "grad_norm": 0.07802223414182663,
      "learning_rate": 1.9562486631120007e-05,
      "loss": 0.1841,
      "step": 144
    },
    {
      "epoch": 3.92991452991453,
      "grad_norm": 0.07316932827234268,
      "learning_rate": 1.9542059095842484e-05,
      "loss": 0.1897,
      "step": 145
    },
    {
      "epoch": 3.9572649572649574,
      "grad_norm": 0.08726699650287628,
      "learning_rate": 1.952117665910714e-05,
      "loss": 0.2426,
      "step": 146
    },
    {
      "epoch": 3.9846153846153847,
      "grad_norm": 0.07561401277780533,
      "learning_rate": 1.9499840316448675e-05,
      "loss": 0.2095,
      "step": 147
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.09810946136713028,
      "learning_rate": 1.9478051085040978e-05,
      "loss": 0.2205,
      "step": 148
    },
    {
      "epoch": 4.027350427350427,
      "grad_norm": 0.0798717737197876,
      "learning_rate": 1.945581000364864e-05,
      "loss": 0.1645,
      "step": 149
    },
    {
      "epoch": 4.0547008547008545,
      "grad_norm": 0.06359215080738068,
      "learning_rate": 1.9433118132577432e-05,
      "loss": 0.1469,
      "step": 150
    },
    {
      "epoch": 4.082051282051282,
      "grad_norm": 0.0767727866768837,
      "learning_rate": 1.9409976553623767e-05,
      "loss": 0.1915,
      "step": 151
    },
    {
      "epoch": 4.109401709401709,
      "grad_norm": 0.07973494380712509,
      "learning_rate": 1.9386386370023104e-05,
      "loss": 0.1917,
      "step": 152
    },
    {
      "epoch": 4.136752136752137,
      "grad_norm": 0.0646173357963562,
      "learning_rate": 1.9362348706397374e-05,
      "loss": 0.1624,
      "step": 153
    },
    {
      "epoch": 4.164102564102564,
      "grad_norm": 0.08949021995067596,
      "learning_rate": 1.933786470870136e-05,
      "loss": 0.2292,
      "step": 154
    },
    {
      "epoch": 4.191452991452992,
      "grad_norm": 0.06604178994894028,
      "learning_rate": 1.931293554416805e-05,
      "loss": 0.1467,
      "step": 155
    },
    {
      "epoch": 4.218803418803419,
      "grad_norm": 0.06303601711988449,
      "learning_rate": 1.9287562401253023e-05,
      "loss": 0.1374,
      "step": 156
    },
    {
      "epoch": 4.246153846153846,
      "grad_norm": 0.08067125082015991,
      "learning_rate": 1.9261746489577767e-05,
      "loss": 0.1466,
      "step": 157
    },
    {
      "epoch": 4.273504273504273,
      "grad_norm": 0.07785424590110779,
      "learning_rate": 1.923548903987201e-05,
      "loss": 0.2073,
      "step": 158
    },
    {
      "epoch": 4.300854700854701,
      "grad_norm": 0.07488624006509781,
      "learning_rate": 1.9208791303915063e-05,
      "loss": 0.186,
      "step": 159
    },
    {
      "epoch": 4.328205128205128,
      "grad_norm": 0.07628554105758667,
      "learning_rate": 1.918165455447614e-05,
      "loss": 0.1742,
      "step": 160
    },
    {
      "epoch": 4.355555555555555,
      "grad_norm": 0.07666824758052826,
      "learning_rate": 1.9154080085253665e-05,
      "loss": 0.1642,
      "step": 161
    },
    {
      "epoch": 4.382905982905983,
      "grad_norm": 0.07676566392183304,
      "learning_rate": 1.912606921081362e-05,
      "loss": 0.1424,
      "step": 162
    },
    {
      "epoch": 4.410256410256411,
      "grad_norm": 0.06923447549343109,
      "learning_rate": 1.909762326652686e-05,
      "loss": 0.136,
      "step": 163
    },
    {
      "epoch": 4.437606837606838,
      "grad_norm": 0.07551904767751694,
      "learning_rate": 1.9068743608505454e-05,
      "loss": 0.1612,
      "step": 164
    },
    {
      "epoch": 4.464957264957265,
      "grad_norm": 0.07246068120002747,
      "learning_rate": 1.9039431613538047e-05,
      "loss": 0.1607,
      "step": 165
    },
    {
      "epoch": 4.492307692307692,
      "grad_norm": 0.07011920213699341,
      "learning_rate": 1.900968867902419e-05,
      "loss": 0.1599,
      "step": 166
    },
    {
      "epoch": 4.51965811965812,
      "grad_norm": 0.06735899299383163,
      "learning_rate": 1.8979516222907776e-05,
      "loss": 0.1233,
      "step": 167
    },
    {
      "epoch": 4.547008547008547,
      "grad_norm": 0.06078854948282242,
      "learning_rate": 1.8948915683609387e-05,
      "loss": 0.134,
      "step": 168
    },
    {
      "epoch": 4.574358974358974,
      "grad_norm": 0.07218386232852936,
      "learning_rate": 1.8917888519957756e-05,
      "loss": 0.1873,
      "step": 169
    },
    {
      "epoch": 4.601709401709401,
      "grad_norm": 0.07262944430112839,
      "learning_rate": 1.8886436211120195e-05,
      "loss": 0.1629,
      "step": 170
    },
    {
      "epoch": 4.629059829059829,
      "grad_norm": 0.06551636010408401,
      "learning_rate": 1.8854560256532098e-05,
      "loss": 0.146,
      "step": 171
    },
    {
      "epoch": 4.656410256410257,
      "grad_norm": 0.06991846859455109,
      "learning_rate": 1.8822262175825463e-05,
      "loss": 0.1721,
      "step": 172
    },
    {
      "epoch": 4.683760683760684,
      "grad_norm": 0.07131160795688629,
      "learning_rate": 1.878954350875641e-05,
      "loss": 0.1493,
      "step": 173
    },
    {
      "epoch": 4.711111111111111,
      "grad_norm": 0.06401313096284866,
      "learning_rate": 1.8756405815131815e-05,
      "loss": 0.1318,
      "step": 174
    },
    {
      "epoch": 4.7384615384615385,
      "grad_norm": 0.07943855226039886,
      "learning_rate": 1.872285067473493e-05,
      "loss": 0.1676,
      "step": 175
    },
    {
      "epoch": 4.765811965811966,
      "grad_norm": 0.06326980143785477,
      "learning_rate": 1.8688879687250067e-05,
      "loss": 0.1037,
      "step": 176
    },
    {
      "epoch": 4.793162393162393,
      "grad_norm": 0.06348279863595963,
      "learning_rate": 1.8654494472186352e-05,
      "loss": 0.1115,
      "step": 177
    },
    {
      "epoch": 4.82051282051282,
      "grad_norm": 0.06488103419542313,
      "learning_rate": 1.8619696668800494e-05,
      "loss": 0.1317,
      "step": 178
    },
    {
      "epoch": 4.8478632478632475,
      "grad_norm": 0.06759411096572876,
      "learning_rate": 1.8584487936018663e-05,
      "loss": 0.1479,
      "step": 179
    },
    {
      "epoch": 4.875213675213676,
      "grad_norm": 0.07329178601503372,
      "learning_rate": 1.854886995235738e-05,
      "loss": 0.1495,
      "step": 180
    },
    {
      "epoch": 4.902564102564103,
      "grad_norm": 0.06753236055374146,
      "learning_rate": 1.8512844415843514e-05,
      "loss": 0.1351,
      "step": 181
    },
    {
      "epoch": 4.92991452991453,
      "grad_norm": 0.0713653713464737,
      "learning_rate": 1.8476413043933316e-05,
      "loss": 0.1545,
      "step": 182
    },
    {
      "epoch": 4.957264957264957,
      "grad_norm": 0.06444072723388672,
      "learning_rate": 1.8439577573430557e-05,
      "loss": 0.1273,
      "step": 183
    },
    {
      "epoch": 4.984615384615385,
      "grad_norm": 0.08061391115188599,
      "learning_rate": 1.8402339760403715e-05,
      "loss": 0.1668,
      "step": 184
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.08009512722492218,
      "learning_rate": 1.8364701380102267e-05,
      "loss": 0.147,
      "step": 185
    },
    {
      "epoch": 5.027350427350427,
      "grad_norm": 0.05948156490921974,
      "learning_rate": 1.8326664226872063e-05,
      "loss": 0.1112,
      "step": 186
    },
    {
      "epoch": 5.0547008547008545,
      "grad_norm": 0.0666678324341774,
      "learning_rate": 1.828823011406977e-05,
      "loss": 0.1521,
      "step": 187
    },
    {
      "epoch": 5.082051282051282,
      "grad_norm": 0.07478218525648117,
      "learning_rate": 1.824940087397641e-05,
      "loss": 0.1856,
      "step": 188
    },
    {
      "epoch": 5.109401709401709,
      "grad_norm": 0.06749894469976425,
      "learning_rate": 1.8210178357710057e-05,
      "loss": 0.1347,
      "step": 189
    },
    {
      "epoch": 5.136752136752137,
      "grad_norm": 0.05957219749689102,
      "learning_rate": 1.8170564435137542e-05,
      "loss": 0.1251,
      "step": 190
    },
    {
      "epoch": 5.164102564102564,
      "grad_norm": 0.0615621916949749,
      "learning_rate": 1.8130560994785325e-05,
      "loss": 0.1295,
      "step": 191
    },
    {
      "epoch": 5.191452991452992,
      "grad_norm": 0.07696539908647537,
      "learning_rate": 1.8090169943749477e-05,
      "loss": 0.1566,
      "step": 192
    },
    {
      "epoch": 5.218803418803419,
      "grad_norm": 0.08989877998828888,
      "learning_rate": 1.8049393207604734e-05,
      "loss": 0.1457,
      "step": 193
    },
    {
      "epoch": 5.246153846153846,
      "grad_norm": 0.05196910351514816,
      "learning_rate": 1.8008232730312724e-05,
      "loss": 0.0979,
      "step": 194
    },
    {
      "epoch": 5.273504273504273,
      "grad_norm": 0.06194616109132767,
      "learning_rate": 1.7966690474129285e-05,
      "loss": 0.1216,
      "step": 195
    },
    {
      "epoch": 5.300854700854701,
      "grad_norm": 0.06810452789068222,
      "learning_rate": 1.7924768419510906e-05,
      "loss": 0.1565,
      "step": 196
    },
    {
      "epoch": 5.328205128205128,
      "grad_norm": 0.0676986575126648,
      "learning_rate": 1.7882468565020327e-05,
      "loss": 0.0969,
      "step": 197
    },
    {
      "epoch": 5.355555555555555,
      "grad_norm": 0.05404294282197952,
      "learning_rate": 1.7839792927231253e-05,
      "loss": 0.0899,
      "step": 198
    },
    {
      "epoch": 5.382905982905983,
      "grad_norm": 0.07869058102369308,
      "learning_rate": 1.7796743540632226e-05,
      "loss": 0.1358,
      "step": 199
    },
    {
      "epoch": 5.410256410256411,
      "grad_norm": 0.0630185604095459,
      "learning_rate": 1.7753322457529615e-05,
      "loss": 0.117,
      "step": 200
    },
    {
      "epoch": 5.437606837606838,
      "grad_norm": 0.05032162368297577,
      "learning_rate": 1.7709531747949796e-05,
      "loss": 0.0752,
      "step": 201
    },
    {
      "epoch": 5.464957264957265,
      "grad_norm": 0.07664351910352707,
      "learning_rate": 1.7665373499540464e-05,
      "loss": 0.1088,
      "step": 202
    },
    {
      "epoch": 5.492307692307692,
      "grad_norm": 0.0728849396109581,
      "learning_rate": 1.7620849817471094e-05,
      "loss": 0.1247,
      "step": 203
    },
    {
      "epoch": 5.51965811965812,
      "grad_norm": 0.05660046264529228,
      "learning_rate": 1.7575962824332595e-05,
      "loss": 0.1031,
      "step": 204
    },
    {
      "epoch": 5.547008547008547,
      "grad_norm": 0.06312074512243271,
      "learning_rate": 1.7530714660036112e-05,
      "loss": 0.1158,
      "step": 205
    },
    {
      "epoch": 5.574358974358974,
      "grad_norm": 0.08237556368112564,
      "learning_rate": 1.7485107481711014e-05,
      "loss": 0.1508,
      "step": 206
    },
    {
      "epoch": 5.601709401709401,
      "grad_norm": 0.06243358552455902,
      "learning_rate": 1.7439143463602052e-05,
      "loss": 0.115,
      "step": 207
    },
    {
      "epoch": 5.629059829059829,
      "grad_norm": 0.0601644329726696,
      "learning_rate": 1.7392824796965703e-05,
      "loss": 0.1229,
      "step": 208
    },
    {
      "epoch": 5.656410256410257,
      "grad_norm": 0.051154233515262604,
      "learning_rate": 1.734615368996573e-05,
      "loss": 0.1092,
      "step": 209
    },
    {
      "epoch": 5.683760683760684,
      "grad_norm": 0.07864390313625336,
      "learning_rate": 1.7299132367567856e-05,
      "loss": 0.1482,
      "step": 210
    },
    {
      "epoch": 5.711111111111111,
      "grad_norm": 0.06971825659275055,
      "learning_rate": 1.7251763071433767e-05,
      "loss": 0.1146,
      "step": 211
    },
    {
      "epoch": 5.7384615384615385,
      "grad_norm": 0.056654565036296844,
      "learning_rate": 1.7204048059814175e-05,
      "loss": 0.0845,
      "step": 212
    },
    {
      "epoch": 5.765811965811966,
      "grad_norm": 0.08310235291719437,
      "learning_rate": 1.715598960744121e-05,
      "loss": 0.1334,
      "step": 213
    },
    {
      "epoch": 5.793162393162393,
      "grad_norm": 0.0663127675652504,
      "learning_rate": 1.710759000541995e-05,
      "loss": 0.0901,
      "step": 214
    },
    {
      "epoch": 5.82051282051282,
      "grad_norm": 0.052724674344062805,
      "learning_rate": 1.7058851561119198e-05,
      "loss": 0.0779,
      "step": 215
    },
    {
      "epoch": 5.8478632478632475,
      "grad_norm": 0.05410235747694969,
      "learning_rate": 1.7009776598061496e-05,
      "loss": 0.0804,
      "step": 216
    },
    {
      "epoch": 5.875213675213676,
      "grad_norm": 0.0614793561398983,
      "learning_rate": 1.6960367455812336e-05,
      "loss": 0.0695,
      "step": 217
    },
    {
      "epoch": 5.902564102564103,
      "grad_norm": 0.05268675088882446,
      "learning_rate": 1.691062648986865e-05,
      "loss": 0.0881,
      "step": 218
    },
    {
      "epoch": 5.92991452991453,
      "grad_norm": 0.06254202872514725,
      "learning_rate": 1.686055607154648e-05,
      "loss": 0.1312,
      "step": 219
    },
    {
      "epoch": 5.957264957264957,
      "grad_norm": 0.05621730163693428,
      "learning_rate": 1.6810158587867973e-05,
      "loss": 0.0933,
      "step": 220
    },
    {
      "epoch": 5.984615384615385,
      "grad_norm": 0.058807067573070526,
      "learning_rate": 1.6759436441447544e-05,
      "loss": 0.1195,
      "step": 221
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.07091236114501953,
      "learning_rate": 1.6708392050377365e-05,
      "loss": 0.0876,
      "step": 222
    },
    {
      "epoch": 6.027350427350427,
      "grad_norm": 0.05832540616393089,
      "learning_rate": 1.6657027848112064e-05,
      "loss": 0.1139,
      "step": 223
    },
    {
      "epoch": 6.0547008547008545,
      "grad_norm": 0.055661190301179886,
      "learning_rate": 1.660534628335273e-05,
      "loss": 0.1334,
      "step": 224
    },
    {
      "epoch": 6.082051282051282,
      "grad_norm": 0.05128062143921852,
      "learning_rate": 1.6553349819930167e-05,
      "loss": 0.1103,
      "step": 225
    },
    {
      "epoch": 6.109401709401709,
      "grad_norm": 0.07236059010028839,
      "learning_rate": 1.6501040936687444e-05,
      "loss": 0.0992,
      "step": 226
    },
    {
      "epoch": 6.136752136752137,
      "grad_norm": 0.04742613434791565,
      "learning_rate": 1.6448422127361707e-05,
      "loss": 0.0998,
      "step": 227
    },
    {
      "epoch": 6.164102564102564,
      "grad_norm": 0.05520625784993172,
      "learning_rate": 1.6395495900465306e-05,
      "loss": 0.0834,
      "step": 228
    },
    {
      "epoch": 6.191452991452992,
      "grad_norm": 0.06846974045038223,
      "learning_rate": 1.63422647791662e-05,
      "loss": 0.1289,
      "step": 229
    },
    {
      "epoch": 6.218803418803419,
      "grad_norm": 0.05256084352731705,
      "learning_rate": 1.6288731301167667e-05,
      "loss": 0.0978,
      "step": 230
    },
    {
      "epoch": 6.246153846153846,
      "grad_norm": 0.05584421381354332,
      "learning_rate": 1.6234898018587336e-05,
      "loss": 0.0878,
      "step": 231
    },
    {
      "epoch": 6.273504273504273,
      "grad_norm": 0.05935710668563843,
      "learning_rate": 1.6180767497835503e-05,
      "loss": 0.1019,
      "step": 232
    },
    {
      "epoch": 6.300854700854701,
      "grad_norm": 0.05887633189558983,
      "learning_rate": 1.6126342319492784e-05,
      "loss": 0.1026,
      "step": 233
    },
    {
      "epoch": 6.328205128205128,
      "grad_norm": 0.06364009529352188,
      "learning_rate": 1.6071625078187113e-05,
      "loss": 0.0954,
      "step": 234
    },
    {
      "epoch": 6.355555555555555,
      "grad_norm": 0.052502237260341644,
      "learning_rate": 1.6016618382470014e-05,
      "loss": 0.0746,
      "step": 235
    },
    {
      "epoch": 6.382905982905983,
      "grad_norm": 0.053162623196840286,
      "learning_rate": 1.5961324854692254e-05,
      "loss": 0.0787,
      "step": 236
    },
    {
      "epoch": 6.410256410256411,
      "grad_norm": 0.053110506385564804,
      "learning_rate": 1.5905747130878853e-05,
      "loss": 0.0768,
      "step": 237
    },
    {
      "epoch": 6.437606837606838,
      "grad_norm": 0.05044591799378395,
      "learning_rate": 1.5849887860603374e-05,
      "loss": 0.0849,
      "step": 238
    },
    {
      "epoch": 6.464957264957265,
      "grad_norm": 0.05466116964817047,
      "learning_rate": 1.5793749706861637e-05,
      "loss": 0.0756,
      "step": 239
    },
    {
      "epoch": 6.492307692307692,
      "grad_norm": 0.05947243049740791,
      "learning_rate": 1.5737335345944758e-05,
      "loss": 0.1087,
      "step": 240
    },
    {
      "epoch": 6.51965811965812,
      "grad_norm": 0.054144397377967834,
      "learning_rate": 1.568064746731156e-05,
      "loss": 0.0859,
      "step": 241
    },
    {
      "epoch": 6.547008547008547,
      "grad_norm": 0.04857293888926506,
      "learning_rate": 1.5623688773460358e-05,
      "loss": 0.0769,
      "step": 242
    },
    {
      "epoch": 6.574358974358974,
      "grad_norm": 0.04789513349533081,
      "learning_rate": 1.556646197980012e-05,
      "loss": 0.0739,
      "step": 243
    },
    {
      "epoch": 6.601709401709401,
      "grad_norm": 0.06391119956970215,
      "learning_rate": 1.5508969814521026e-05,
      "loss": 0.1284,
      "step": 244
    },
    {
      "epoch": 6.629059829059829,
      "grad_norm": 0.05887790769338608,
      "learning_rate": 1.5451215018464386e-05,
      "loss": 0.0899,
      "step": 245
    },
    {
      "epoch": 6.656410256410257,
      "grad_norm": 0.05215389281511307,
      "learning_rate": 1.5393200344991993e-05,
      "loss": 0.0632,
      "step": 246
    },
    {
      "epoch": 6.683760683760684,
      "grad_norm": 0.055356189608573914,
      "learning_rate": 1.533492855985485e-05,
      "loss": 0.07,
      "step": 247
    },
    {
      "epoch": 6.711111111111111,
      "grad_norm": 0.04649018123745918,
      "learning_rate": 1.527640244106133e-05,
      "loss": 0.0788,
      "step": 248
    },
    {
      "epoch": 6.7384615384615385,
      "grad_norm": 0.05361083522439003,
      "learning_rate": 1.5217624778744718e-05,
      "loss": 0.0876,
      "step": 249
    },
    {
      "epoch": 6.765811965811966,
      "grad_norm": 0.0436396449804306,
      "learning_rate": 1.5158598375030218e-05,
      "loss": 0.0508,
      "step": 250
    },
    {
      "epoch": 6.793162393162393,
      "grad_norm": 0.05656812712550163,
      "learning_rate": 1.5099326043901361e-05,
      "loss": 0.1102,
      "step": 251
    },
    {
      "epoch": 6.82051282051282,
      "grad_norm": 0.05854276195168495,
      "learning_rate": 1.503981061106584e-05,
      "loss": 0.1111,
      "step": 252
    },
    {
      "epoch": 6.8478632478632475,
      "grad_norm": 0.0520724318921566,
      "learning_rate": 1.4980054913820814e-05,
      "loss": 0.0759,
      "step": 253
    },
    {
      "epoch": 6.875213675213676,
      "grad_norm": 0.052834998816251755,
      "learning_rate": 1.4920061800917637e-05,
      "loss": 0.071,
      "step": 254
    },
    {
      "epoch": 6.902564102564103,
      "grad_norm": 0.05364437401294708,
      "learning_rate": 1.485983413242606e-05,
      "loss": 0.0637,
      "step": 255
    },
    {
      "epoch": 6.92991452991453,
      "grad_norm": 0.05202316492795944,
      "learning_rate": 1.4799374779597866e-05,
      "loss": 0.0814,
      "step": 256
    },
    {
      "epoch": 6.957264957264957,
      "grad_norm": 0.046544890850782394,
      "learning_rate": 1.4738686624729987e-05,
      "loss": 0.0548,
      "step": 257
    },
    {
      "epoch": 6.984615384615385,
      "grad_norm": 0.06124790757894516,
      "learning_rate": 1.4677772561027121e-05,
      "loss": 0.0914,
      "step": 258
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.06870914250612259,
      "learning_rate": 1.4616635492463775e-05,
      "loss": 0.0896,
      "step": 259
    },
    {
      "epoch": 7.027350427350427,
      "grad_norm": 0.04707810655236244,
      "learning_rate": 1.4555278333645833e-05,
      "loss": 0.0634,
      "step": 260
    },
    {
      "epoch": 7.0547008547008545,
      "grad_norm": 0.047469377517700195,
      "learning_rate": 1.4493704009671614e-05,
      "loss": 0.0748,
      "step": 261
    },
    {
      "epoch": 7.082051282051282,
      "grad_norm": 0.05322905629873276,
      "learning_rate": 1.4431915455992416e-05,
      "loss": 0.0743,
      "step": 262
    },
    {
      "epoch": 7.109401709401709,
      "grad_norm": 0.05060707405209541,
      "learning_rate": 1.4369915618272568e-05,
      "loss": 0.0792,
      "step": 263
    },
    {
      "epoch": 7.136752136752137,
      "grad_norm": 0.0476590096950531,
      "learning_rate": 1.4307707452249013e-05,
      "loss": 0.0607,
      "step": 264
    },
    {
      "epoch": 7.164102564102564,
      "grad_norm": 0.052831731736660004,
      "learning_rate": 1.424529392359039e-05,
      "loss": 0.0594,
      "step": 265
    },
    {
      "epoch": 7.191452991452992,
      "grad_norm": 0.048133719712495804,
      "learning_rate": 1.4182678007755653e-05,
      "loss": 0.0736,
      "step": 266
    },
    {
      "epoch": 7.218803418803419,
      "grad_norm": 0.0521954670548439,
      "learning_rate": 1.4119862689852224e-05,
      "loss": 0.077,
      "step": 267
    },
    {
      "epoch": 7.246153846153846,
      "grad_norm": 0.05551777780056,
      "learning_rate": 1.4056850964493668e-05,
      "loss": 0.0822,
      "step": 268
    },
    {
      "epoch": 7.273504273504273,
      "grad_norm": 0.0486263632774353,
      "learning_rate": 1.3993645835656955e-05,
      "loss": 0.0668,
      "step": 269
    },
    {
      "epoch": 7.300854700854701,
      "grad_norm": 0.06206909194588661,
      "learning_rate": 1.3930250316539237e-05,
      "loss": 0.0685,
      "step": 270
    },
    {
      "epoch": 7.328205128205128,
      "grad_norm": 0.054594993591308594,
      "learning_rate": 1.3866667429414188e-05,
      "loss": 0.0636,
      "step": 271
    },
    {
      "epoch": 7.355555555555555,
      "grad_norm": 0.061380352824926376,
      "learning_rate": 1.3802900205487948e-05,
      "loss": 0.0805,
      "step": 272
    },
    {
      "epoch": 7.382905982905983,
      "grad_norm": 0.04533236473798752,
      "learning_rate": 1.3738951684754585e-05,
      "loss": 0.0725,
      "step": 273
    },
    {
      "epoch": 7.410256410256411,
      "grad_norm": 0.049940790981054306,
      "learning_rate": 1.3674824915851193e-05,
      "loss": 0.0666,
      "step": 274
    },
    {
      "epoch": 7.437606837606838,
      "grad_norm": 0.049043238162994385,
      "learning_rate": 1.3610522955912551e-05,
      "loss": 0.0743,
      "step": 275
    },
    {
      "epoch": 7.464957264957265,
      "grad_norm": 0.06057141721248627,
      "learning_rate": 1.3546048870425356e-05,
      "loss": 0.0837,
      "step": 276
    },
    {
      "epoch": 7.492307692307692,
      "grad_norm": 0.05926928669214249,
      "learning_rate": 1.3481405733082118e-05,
      "loss": 0.0915,
      "step": 277
    },
    {
      "epoch": 7.51965811965812,
      "grad_norm": 0.054751597344875336,
      "learning_rate": 1.3416596625634595e-05,
      "loss": 0.0732,
      "step": 278
    },
    {
      "epoch": 7.547008547008547,
      "grad_norm": 0.05675365775823593,
      "learning_rate": 1.3351624637746885e-05,
      "loss": 0.0866,
      "step": 279
    },
    {
      "epoch": 7.574358974358974,
      "grad_norm": 0.04880613461136818,
      "learning_rate": 1.3286492866848143e-05,
      "loss": 0.0543,
      "step": 280
    },
    {
      "epoch": 7.601709401709401,
      "grad_norm": 0.06596653908491135,
      "learning_rate": 1.3221204417984907e-05,
      "loss": 0.1037,
      "step": 281
    },
    {
      "epoch": 7.629059829059829,
      "grad_norm": 0.049215514212846756,
      "learning_rate": 1.3155762403673065e-05,
      "loss": 0.062,
      "step": 282
    },
    {
      "epoch": 7.656410256410257,
      "grad_norm": 0.055452704429626465,
      "learning_rate": 1.3090169943749475e-05,
      "loss": 0.0637,
      "step": 283
    },
    {
      "epoch": 7.683760683760684,
      "grad_norm": 0.056469619274139404,
      "learning_rate": 1.3024430165223245e-05,
      "loss": 0.0804,
      "step": 284
    },
    {
      "epoch": 7.711111111111111,
      "grad_norm": 0.05470544472336769,
      "learning_rate": 1.2958546202126638e-05,
      "loss": 0.0663,
      "step": 285
    },
    {
      "epoch": 7.7384615384615385,
      "grad_norm": 0.0502200722694397,
      "learning_rate": 1.2892521195365679e-05,
      "loss": 0.0657,
      "step": 286
    },
    {
      "epoch": 7.765811965811966,
      "grad_norm": 0.055291999131441116,
      "learning_rate": 1.2826358292570398e-05,
      "loss": 0.0904,
      "step": 287
    },
    {
      "epoch": 7.793162393162393,
      "grad_norm": 0.052090834826231,
      "learning_rate": 1.2760060647944794e-05,
      "loss": 0.0766,
      "step": 288
    },
    {
      "epoch": 7.82051282051282,
      "grad_norm": 0.05227462947368622,
      "learning_rate": 1.2693631422116455e-05,
      "loss": 0.0675,
      "step": 289
    },
    {
      "epoch": 7.8478632478632475,
      "grad_norm": 0.05366664379835129,
      "learning_rate": 1.262707378198587e-05,
      "loss": 0.0992,
      "step": 290
    },
    {
      "epoch": 7.875213675213676,
      "grad_norm": 0.054356735199689865,
      "learning_rate": 1.2560390900575472e-05,
      "loss": 0.0638,
      "step": 291
    },
    {
      "epoch": 7.902564102564103,
      "grad_norm": 0.04864858090877533,
      "learning_rate": 1.2493585956878354e-05,
      "loss": 0.0602,
      "step": 292
    },
    {
      "epoch": 7.92991452991453,
      "grad_norm": 0.05671938136219978,
      "learning_rate": 1.242666213570672e-05,
      "loss": 0.0782,
      "step": 293
    },
    {
      "epoch": 7.957264957264957,
      "grad_norm": 0.052807822823524475,
      "learning_rate": 1.2359622627540059e-05,
      "loss": 0.0892,
      "step": 294
    },
    {
      "epoch": 7.984615384615385,
      "grad_norm": 0.054464250802993774,
      "learning_rate": 1.229247062837304e-05,
      "loss": 0.0833,
      "step": 295
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.06655165553092957,
      "learning_rate": 1.2225209339563144e-05,
      "loss": 0.0613,
      "step": 296
    }
  ],
  "logging_steps": 1,
  "max_steps": 555,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.5878869092479795e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
